import os

# chroma ä¸ä¸Šä¼ æ•°æ®
os.environ["ANONYMIZED_TELEMETRY"] = "false"
os.environ["DISABLE_TELEMETRY"] = "1"
os.environ["CHROMA_TELEMETRY_ENABLED"] = "false"

import json
import logging
import pandas as pd
from typing import Any, Dict, List

# langchain
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate
from langchain_ollama import OllamaLLM, OllamaEmbeddings

# llama-index & chroma
import chromadb
from llama_index.core import Settings  # å…¨å±€
from llama_index.core import Document
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext
from llama_index.vector_stores.chroma import ChromaVectorStore  # æ³¨æ„å¯¼å…¥è·¯å¾„

# å¯¼å…¥é¢†åŸŸçŸ¥è¯†
from domain_knowledge import (
    get_error_code_meaning, 
    get_service_dependencies, 
    get_fault_category,
    get_severity_level,
    get_common_pattern_info,
    get_monitoring_recommendations,
    get_expert_insights,
    get_industry_standards,
    get_best_practices,
    FAULT_CATEGORIES,
    COMMON_PATTERNS
)

# æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TopKLogSystem:
    def __init__(
            self,
            log_path: str,
            llm: str,
            embedding_model: str,
    ) -> None:
        # init models
        self.embedding_model = OllamaEmbeddings(model=embedding_model)

        self.llm = OllamaLLM(model=llm, temperature=0.1)

        # init database
        Settings.llm = self.llm
        Settings.embed_model = self.embedding_model  # å…¨å±€è®¾ç½®

        self.log_path = log_path
        self.log_index = None
        self.vector_store = None
        self._build_vectorstore()  # ç›´æ¥æ„å»º

    # åŠ è½½æ•°æ®å¹¶æ„å»ºç´¢å¼•
    def _build_vectorstore(self):
        vector_store_path = "./data/vector_stores"
        os.makedirs(vector_store_path, exist_ok=True)  # exist_ok=True ç›®å½•å­˜åœ¨æ—¶ä¸æŠ¥é”™

        chroma_client = chromadb.PersistentClient(path=vector_store_path)  # chromadb æŒä¹…åŒ–

        # ChromaVectorStore å°† collection ä¸ store ç»‘å®š
        # ä¹Ÿæ˜¯å°† Chroma åŒ…è£…ä¸º llama-index çš„æ¥å£
        # StorageContextå­˜å‚¨ä¸Šä¸‹æ–‡ï¼Œ åŒ…å« Vector Storeã€Document Storeã€Index Store ç­‰
        log_collection = chroma_client.get_or_create_collection("log_collection")

        # æ„å»º log åº“ index
        log_vector_store = ChromaVectorStore(chroma_collection=log_collection)
        log_storage_context = StorageContext.from_defaults(vector_store=log_vector_store)
        if log_documents := self._load_documents(self.log_path):
            self.log_index = VectorStoreIndex.from_documents(
                log_documents,
                storage_context=log_storage_context,
                show_progress=True,
            )
            logger.info(f"æ—¥å¿—åº“ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(log_documents)} æ¡æ—¥å¿—")

    @staticmethod
    # åŠ è½½æ–‡æ¡£æ•°æ®
    def _load_documents(data_path: str) -> List[Document]:
        if not os.path.exists(data_path):
            logger.warning(f"æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {data_path}")
            return []

        documents = []
        for file in os.listdir(data_path):
            ext = os.path.splitext(file)[1]
            if ext not in [".txt", ".md", ".json", ".jsonl", ".csv"]:
                continue

            file_path = f"{data_path}/{file}"
            try:
                if ext == ".csv":  # utf-8 çš„ csv
                    # å¤§å‹ csv åˆ†å—è¿›è¡Œè¯»å–
                    chunk_size = 1000  # æ¯æ¬¡è¯»å–1000è¡Œ
                    for chunk in pd.read_csv(file_path, chunksize=chunk_size):
                        for row in chunk.itertuples(index=False):  # æ— è¡Œå·
                            content = str(row).replace("Pandas", " ")
                            documents.append(Document(text=content))
                else:  # .txt or .md, .json
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        doc = Document(text=content, )
                        documents.append(doc)
            except Exception as e:
                logger.error(f"åŠ è½½æ–‡æ¡£å¤±è´¥ {file_path}: {e}")
        return documents

        # æ£€ç´¢ç›¸å…³æ—¥å¿—

    def retrieve_logs(self, query: str, top_k: int = 10) -> List[Dict]:
        """
        å¤šç­–ç•¥æ™ºèƒ½æ£€ç´¢æ—¥å¿—
        """
        if not self.log_index:
            return []

        try:
            # ç­–ç•¥1: è¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢
            semantic_results = self._semantic_retrieval(query, top_k)
            
            # ç­–ç•¥2: å…³é”®è¯ç²¾ç¡®åŒ¹é…
            keyword_results = self._keyword_retrieval(query, top_k)
            
            # ç­–ç•¥3: é”™è¯¯ç åŒ¹é…
            error_code_results = self._error_code_retrieval(query, top_k)
            
            # åˆå¹¶å’Œå»é‡ç»“æœ
            all_results = semantic_results + keyword_results + error_code_results
            filtered_results = self._deduplicate_and_rank(all_results, top_k)
            
            return filtered_results
        except Exception as e:
            logger.error(f"æ—¥å¿—æ£€ç´¢å¤±è´¥: {e}")
            return []

    def _semantic_retrieval(self, query: str, top_k: int) -> List[Dict]:
        """è¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢"""
        try:
            retriever = self.log_index.as_retriever(similarity_top_k=top_k)
            results = retriever.retrieve(query)
            
            formatted_results = []
            for result in results:
                formatted_results.append({
                    "content": result.text,
                    "score": result.score,
                    "retrieval_method": "semantic"
                })
            return formatted_results
        except Exception as e:
            logger.error(f"è¯­ä¹‰æ£€ç´¢å¤±è´¥: {e}")
            return []

    def _keyword_retrieval(self, query: str, top_k: int) -> List[Dict]:
        """å…³é”®è¯ç²¾ç¡®åŒ¹é…æ£€ç´¢"""
        try:
            # æå–æŸ¥è¯¢ä¸­çš„å…³é”®è¯
            keywords = self._extract_keywords(query)
            if not keywords:
                return []
            
            # ä½¿ç”¨å…³é”®è¯è¿›è¡Œæ£€ç´¢
            keyword_query = " ".join(keywords)
            retriever = self.log_index.as_retriever(similarity_top_k=top_k)
            results = retriever.retrieve(keyword_query)
            
            formatted_results = []
            for result in results:
                # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
                keyword_score = self._calculate_keyword_score(result.text, keywords)
                if keyword_score > 0.3:  # å…³é”®è¯åŒ¹é…é˜ˆå€¼
                    formatted_results.append({
                        "content": result.text,
                        "score": keyword_score,
                        "retrieval_method": "keyword"
                    })
            return formatted_results
        except Exception as e:
            logger.error(f"å…³é”®è¯æ£€ç´¢å¤±è´¥: {e}")
            return []

    def _error_code_retrieval(self, query: str, top_k: int) -> List[Dict]:
        """é”™è¯¯ç ç²¾ç¡®åŒ¹é…æ£€ç´¢"""
        try:
            # æå–æŸ¥è¯¢ä¸­çš„é”™è¯¯ç 
            error_codes = self._extract_error_codes(query)
            if not error_codes:
                return []
            
            # ä½¿ç”¨é”™è¯¯ç è¿›è¡Œæ£€ç´¢
            error_query = " ".join(error_codes)
            retriever = self.log_index.as_retriever(similarity_top_k=top_k)
            results = retriever.retrieve(error_query)
            
            formatted_results = []
            for result in results:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«é”™è¯¯ç 
                if any(code in result.text for code in error_codes):
                    formatted_results.append({
                        "content": result.text,
                        "score": 1.0,  # ç²¾ç¡®åŒ¹é…ç»™æœ€é«˜åˆ†
                        "retrieval_method": "error_code"
                    })
            return formatted_results
        except Exception as e:
            logger.error(f"é”™è¯¯ç æ£€ç´¢å¤±è´¥: {e}")
            return []

    def _extract_keywords(self, query: str) -> List[str]:
        """æå–æŸ¥è¯¢å…³é”®è¯"""
        import re
        # æå–ä¸­æ–‡å’Œè‹±æ–‡å…³é”®è¯
        keywords = []
        
        # æå–ä¸­æ–‡è¯æ±‡ï¼ˆ2-4ä¸ªå­—ç¬¦ï¼‰
        chinese_words = re.findall(r'[\u4e00-\u9fff]{2,4}', query)
        keywords.extend(chinese_words)
        
        # æå–è‹±æ–‡è¯æ±‡ï¼ˆ3ä¸ªå­—ç¬¦ä»¥ä¸Šï¼‰
        english_words = re.findall(r'\b[A-Za-z]{3,}\b', query)
        keywords.extend(english_words)
        
        # æå–æŠ€æœ¯æœ¯è¯­
        tech_terms = ['æ•°æ®åº“', 'è¿æ¥æ± ', 'è®¤è¯', 'æ”¯ä»˜', 'åº“å­˜', 'è®¢å•', 'ç”¨æˆ·', 'æœåŠ¡']
        for term in tech_terms:
            if term in query:
                keywords.append(term)
        
        return list(set(keywords))  # å»é‡

    def _calculate_keyword_score(self, text: str, keywords: List[str]) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
        if not keywords:
            return 0.0
        
        matches = 0
        for keyword in keywords:
            if keyword.lower() in text.lower():
                matches += 1
        
        return matches / len(keywords)

    def _deduplicate_and_rank(self, all_results: List[Dict], top_k: int) -> List[Dict]:
        """å»é‡å’Œæ’åºç»“æœ"""
        # æŒ‰å†…å®¹å»é‡
        seen_contents = set()
        unique_results = []
        
        for result in all_results:
            content_hash = hash(result["content"])
            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                unique_results.append(result)
        
        # æŒ‰åˆ†æ•°æ’åº
        unique_results.sort(key=lambda x: x["score"], reverse=True)
        
        # è¿”å›å‰top_kä¸ªç»“æœ
        return unique_results[:top_k]

            # LLM ç”Ÿæˆå“åº”

    def generate_response(self, query: str, context: Dict) -> str:
        prompt = self._build_prompt(query, context)  # æ„å»ºæç¤ºè¯

        try:
            response = self.llm.invoke(prompt)  # è°ƒç”¨LLM
            return response
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            return f"ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}"

            # æ„å»º prompt

    def _build_prompt(self, query: str, context: Dict) -> List[Dict]:
        # æ„å»ºé¢†åŸŸçŸ¥è¯†ä¸Šä¸‹æ–‡
        domain_context = self._build_domain_context(context)
        
        # ç³»ç»Ÿæ¶ˆæ¯ - å®šä¹‰ä¸“ä¸šè§’è‰²å’Œåˆ†ææ¡†æ¶
        system_message = SystemMessagePromptTemplate.from_template(f"""
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç”µå•†ç³»ç»Ÿæ•…éšœè¯Šæ–­ä¸“å®¶ï¼Œå…·æœ‰ä¸°å¯Œçš„æ—¥å¿—åˆ†æå’Œæ•…éšœæ’æŸ¥ç»éªŒã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºæä¾›çš„æ—¥å¿—ä¿¡æ¯ï¼Œè¿›è¡Œä¸“ä¸šçš„æ•…éšœåˆ†æã€‚

## é¢†åŸŸçŸ¥è¯†èƒŒæ™¯
{domain_context}

## åˆ†ææ¡†æ¶
è¯·æŒ‰ç…§ä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤è¿›è¡Œç»“æ„åŒ–åˆ†æï¼š

### ç¬¬ä¸€æ­¥ï¼šæ•…éšœç°è±¡è¯†åˆ«
- è¯†åˆ«æ—¥å¿—ä¸­çš„é”™è¯¯çº§åˆ«ï¼ˆFATAL/ERROR/WARN/INFOï¼‰
- æå–å…³é”®é”™è¯¯ç å’ŒæœåŠ¡åç§°ï¼Œç»“åˆé¢†åŸŸçŸ¥è¯†ç†è§£å…¶å«ä¹‰
- åˆ†ææ•…éšœå‘ç”Ÿçš„æ—¶é—´æ¨¡å¼å’Œé¢‘ç‡
- è¯„ä¼°æ•…éšœçš„ä¸¥é‡ç¨‹åº¦å’Œå½±å“èŒƒå›´

### ç¬¬äºŒæ­¥ï¼šæ ¹å› åˆ†æ
- åŸºäºé”™è¯¯ç å’Œæ—¥å¿—å†…å®¹åˆ†æå¯èƒ½çš„æ ¹æœ¬åŸå› 
- è€ƒè™‘æœåŠ¡é—´çš„ä¾èµ–å…³ç³»å’Œè°ƒç”¨é“¾
- è¯†åˆ«è§¦å‘æ•…éšœçš„æ¡ä»¶å’Œç¯å¢ƒå› ç´ 
- ç»“åˆå¸¸è§æ•…éšœæ¨¡å¼æä¾›ç½®ä¿¡åº¦è¯„ä¼°ï¼ˆé«˜/ä¸­/ä½ï¼‰

### ç¬¬ä¸‰æ­¥ï¼šè§£å†³æ–¹æ¡ˆå»ºè®®
- æä¾›ç«‹å³ä¿®å¤æªæ–½ï¼ˆç´§æ€¥å¤„ç†ï¼‰
- å»ºè®®é•¿æœŸä¼˜åŒ–æ–¹æ¡ˆï¼ˆæ ¹æœ¬è§£å†³ï¼‰
- ç»™å‡ºé¢„é˜²æªæ–½å’Œç›‘æ§å»ºè®®
- æŒ‰ä¼˜å…ˆçº§æ’åºè§£å†³æ–¹æ¡ˆ

## è¾“å‡ºæ ¼å¼è¦æ±‚
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼Œç¡®ä¿ç»“æ„å®Œæ•´ï¼š

JSONæ ¼å¼è¦æ±‚ï¼š
- fault_summary: åŒ…å«severity("HIGH"/"MEDIUM"/"LOW")ã€category("AUTHENTICATION"/"PAYMENT"/"DATABASE"/"INVENTORY"/"SYSTEM_RESOURCE"/"NETWORK")ã€description(å­—ç¬¦ä¸²)ã€affected_services(å­—ç¬¦ä¸²æ•°ç»„)ã€error_codes(å­—ç¬¦ä¸²æ•°ç»„)
- root_cause_analysis: åŒ…å«primary_cause(å­—ç¬¦ä¸²)ã€contributing_factors(å­—ç¬¦ä¸²æ•°ç»„)ã€confidence_level("HIGH"/"MEDIUM"/"LOW")ã€reasoning(å­—ç¬¦ä¸²)
- solutions: åŒ…å«immediate_actions(å¯¹è±¡æ•°ç»„)ã€long_term_fixes(å¯¹è±¡æ•°ç»„)ã€prevention_measures(å¯¹è±¡æ•°ç»„)
- monitoring_recommendations: å­—ç¬¦ä¸²æ•°ç»„

æ¯ä¸ªè§£å†³æ–¹æ¡ˆå¯¹è±¡å¿…é¡»åŒ…å«action(å­—ç¬¦ä¸²)å’Œpriority("HIGH"/"MEDIUM"/"LOW")å­—æ®µã€‚

é‡è¦ï¼šç›´æ¥è¾“å‡ºJSONå†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•markdownä»£ç å—æ ‡è®°ã€è§£é‡Šæ–‡å­—æˆ–å…¶ä»–æ ¼å¼ã€‚
        """)

        # æ„å»ºç»“æ„åŒ–çš„æ—¥å¿—ä¸Šä¸‹æ–‡
        log_context = self._build_structured_context(context)

        # ç”¨æˆ·æ¶ˆæ¯ - æ˜ç¡®åˆ†æä»»åŠ¡
        user_message = HumanMessagePromptTemplate.from_template("""
## ç›¸å…³æ—¥å¿—ä¿¡æ¯
{log_context}

## åˆ†æä»»åŠ¡
ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·åŸºäºä»¥ä¸Šæ—¥å¿—ä¿¡æ¯ï¼ŒæŒ‰ç…§åˆ†ææ¡†æ¶è¿›è¡Œä¸“ä¸šçš„æ•…éšœè¯Šæ–­åˆ†æã€‚

è¾“å‡ºè¦æ±‚ï¼š
1. ç›´æ¥è¾“å‡ºJSONæ ¼å¼çš„åˆ†æç»“æœ
2. ä¸è¦åŒ…å«ä»»ä½•markdownä»£ç å—æ ‡è®°ï¼ˆå¦‚```jsonï¼‰
3. ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—æˆ–é¢å¤–å†…å®¹
4. ç¡®ä¿JSONæ ¼å¼å®Œå…¨æ­£ç¡®ï¼Œå¯ä»¥è¢«ç›´æ¥è§£æ
        """)

        # åˆ›å»ºæç¤ºè¯
        prompt = ChatPromptTemplate.from_messages([
            system_message,
            user_message
        ])

        return prompt.format_prompt(
            log_context=log_context,
            query=query
        ).to_messages()

    def _build_structured_context(self, context: List[Dict]) -> str:
        """
        æ„å»ºæ™ºèƒ½åŒ–çš„æ—¥å¿—ä¸Šä¸‹æ–‡ï¼Œæé«˜ä¿¡æ¯åˆ©ç”¨æ•ˆç‡
        """
        if not context:
            return "æœªæ‰¾åˆ°ç›¸å…³æ—¥å¿—ä¿¡æ¯ã€‚"
        
        # æ™ºèƒ½è¿‡æ»¤å’Œæ’åº
        filtered_context = self._intelligent_context_filter(context)
        
        log_context = "## ç›¸å…³æ—¥å¿—ä¿¡æ¯\n\n"
        
        # æŒ‰æ£€ç´¢æ–¹æ³•å’Œç›¸å…³æ€§åˆ†ç»„
        semantic_logs = [log for log in filtered_context if log.get('retrieval_method') == 'semantic']
        keyword_logs = [log for log in filtered_context if log.get('retrieval_method') == 'keyword']
        error_code_logs = [log for log in filtered_context if log.get('retrieval_method') == 'error_code']
        
        # ä¼˜å…ˆæ˜¾ç¤ºé”™è¯¯ç ç²¾ç¡®åŒ¹é…çš„æ—¥å¿—
        if error_code_logs:
            log_context += "### ğŸ” ç²¾ç¡®åŒ¹é…çš„æ—¥å¿—\n"
            for i, log in enumerate(error_code_logs[:3], 1):
                log_context += self._format_log_entry(log, i, "ç²¾ç¡®åŒ¹é…")
        
        # æ˜¾ç¤ºå…³é”®è¯åŒ¹é…çš„æ—¥å¿—
        if keyword_logs:
            log_context += "\n### ğŸ”‘ å…³é”®è¯åŒ¹é…çš„æ—¥å¿—\n"
            for i, log in enumerate(keyword_logs[:3], 1):
                log_context += self._format_log_entry(log, i, "å…³é”®è¯åŒ¹é…")
        
        # æ˜¾ç¤ºè¯­ä¹‰ç›¸ä¼¼çš„æ—¥å¿—
        if semantic_logs:
            log_context += "\n### ğŸ§  è¯­ä¹‰ç›¸ä¼¼çš„æ—¥å¿—\n"
            for i, log in enumerate(semantic_logs[:3], 1):
                log_context += self._format_log_entry(log, i, "è¯­ä¹‰ç›¸ä¼¼")
        
        return log_context

    def _intelligent_context_filter(self, context: List[Dict]) -> List[Dict]:
        """
        æ™ºèƒ½ä¸Šä¸‹æ–‡è¿‡æ»¤
        """
        filtered_logs = []
        
        for log in context:
            content = log.get('content', '')
            score = log.get('score', 0)
            
            # è¿‡æ»¤æ¡ä»¶
            if score < 0.1:  # ç›¸å…³æ€§å¤ªä½
                continue
            
            # æå–å…³é”®ä¿¡æ¯
            log_level = self._extract_log_level(content)
            error_codes = self._extract_error_codes(content)
            services = self._extract_services(content)
            
            # è®¡ç®—ä¿¡æ¯ä»·å€¼åˆ†æ•°
            value_score = self._calculate_information_value(content, log_level, error_codes, services)
            
            if value_score > 0.3:  # ä¿¡æ¯ä»·å€¼é˜ˆå€¼
                log['value_score'] = value_score
                log['log_level'] = log_level
                log['error_codes'] = error_codes
                log['services'] = services
                filtered_logs.append(log)
        
        # æŒ‰ä»·å€¼åˆ†æ•°å’Œç›¸å…³æ€§åˆ†æ•°ç»¼åˆæ’åº
        filtered_logs.sort(key=lambda x: (x.get('value_score', 0) * 0.6 + x.get('score', 0) * 0.4), reverse=True)
        
        return filtered_logs[:8]  # é™åˆ¶æœ€å¤š8æ¡

    def _calculate_information_value(self, content: str, log_level: str, error_codes: List[str], services: List[str]) -> float:
        """
        è®¡ç®—æ—¥å¿—ä¿¡æ¯ä»·å€¼åˆ†æ•°
        """
        value_score = 0.0
        
        # æ—¥å¿—çº§åˆ«æƒé‡
        level_weights = {'FATAL': 1.0, 'ERROR': 0.8, 'WARN': 0.6, 'INFO': 0.4, 'DEBUG': 0.2}
        value_score += level_weights.get(log_level, 0.1)
        
        # é”™è¯¯ç æƒé‡
        if error_codes:
            value_score += 0.3
        
        # æœåŠ¡åç§°æƒé‡
        if services:
            value_score += 0.2
        
        # å†…å®¹é•¿åº¦æƒé‡ï¼ˆé¿å…è¿‡çŸ­æˆ–è¿‡é•¿çš„æ—¥å¿—ï¼‰
        content_length = len(content)
        if 50 <= content_length <= 500:
            value_score += 0.1
        
        return min(value_score, 1.0)  # é™åˆ¶æœ€å¤§å€¼ä¸º1.0

    def _format_log_entry(self, log: Dict, index: int, match_type: str) -> str:
        """
        æ ¼å¼åŒ–æ—¥å¿—æ¡ç›®
        """
        content = log.get('content', '')
        score = log.get('score', 0)
        log_level = log.get('log_level', 'UNKNOWN')
        error_codes = log.get('error_codes', [])
        services = log.get('services', [])
        
        formatted_entry = f"#### æ—¥å¿— {index} ({match_type}, ç›¸å…³æ€§: {score:.3f})\n"
        formatted_entry += f"**çº§åˆ«**: {log_level}\n"
        
        if error_codes:
            formatted_entry += f"**é”™è¯¯ç **: {', '.join(error_codes)}\n"
        if services:
            formatted_entry += f"**æ¶‰åŠæœåŠ¡**: {', '.join(services)}\n"
        
        formatted_entry += f"**å†…å®¹**: {content}\n\n"
        
        return formatted_entry

    def _build_domain_context(self, context: List[Dict]) -> str:
        """
        æ„å»ºé¢†åŸŸçŸ¥è¯†ä¸Šä¸‹æ–‡ï¼Œä¸ºAIæä¾›ä¸“ä¸šçš„æ•…éšœè¯Šæ–­çŸ¥è¯†
        """
        if not context:
            return "æœªæ‰¾åˆ°ç›¸å…³æ—¥å¿—ä¿¡æ¯ã€‚"
        
        # æå–æ‰€æœ‰é”™è¯¯ç å’ŒæœåŠ¡
        error_codes = set()
        services = set()
        
        for log in context:
            content = log.get('content', '')
            error_codes.update(self._extract_error_codes(content))
            services.update(self._extract_services(content))
        
        # æ„å»ºé¢†åŸŸçŸ¥è¯†ä¸Šä¸‹æ–‡
        domain_context = "## ç›¸å…³é”™è¯¯ç çš„ä¸“ä¸šçŸ¥è¯†\n"
        
        for error_code in list(error_codes)[:10]:  # é™åˆ¶æœ€å¤š10ä¸ªé”™è¯¯ç 
            meaning = get_error_code_meaning(error_code)
            category = get_fault_category(error_code)
            severity = get_severity_level(error_code)
            
            domain_context += f"- **{error_code}**: {meaning}\n"
            domain_context += f"  - åˆ†ç±»: {category}\n"
            domain_context += f"  - ä¸¥é‡ç¨‹åº¦: {severity}\n"
        
        # æ·»åŠ æœåŠ¡ä¾èµ–å…³ç³»
        if services:
            domain_context += "\n## æœåŠ¡ä¾èµ–å…³ç³»\n"
            for service in list(services)[:5]:  # é™åˆ¶æœ€å¤š5ä¸ªæœåŠ¡
                dependencies = get_service_dependencies(service)
                if dependencies:
                    domain_context += f"- **{service}** ä¾èµ–: {', '.join(dependencies)}\n"
        
        # æ·»åŠ å¸¸è§æ•…éšœæ¨¡å¼
        domain_context += "\n## å¸¸è§æ•…éšœæ¨¡å¼\n"
        for pattern_name, pattern_info in list(COMMON_PATTERNS.items())[:3]:  # é™åˆ¶æœ€å¤š3ä¸ªæ¨¡å¼
            domain_context += f"- **{pattern_name}**:\n"
            domain_context += f"  - ç—‡çŠ¶: {', '.join(pattern_info.get('symptoms', [])[:3])}\n"
            domain_context += f"  - å¸¸è§åŸå› : {', '.join(pattern_info.get('root_causes', [])[:3])}\n"
            domain_context += f"  - ç«‹å³è¡ŒåŠ¨: {', '.join(pattern_info.get('immediate_actions', [])[:2])}\n"
        
        # æ·»åŠ ä¸“å®¶æ´å¯Ÿå’Œè¡Œä¸šæ ‡å‡†
        domain_context += "\n## ä¸“å®¶æ´å¯Ÿå’Œè¡Œä¸šæ ‡å‡†\n"
        expert_patterns = ["æ•°æ®åº“è¿æ¥æ± è€—å°½", "è®¤è¯å¤±è´¥", "æ”¯ä»˜å¼‚å¸¸"]
        for pattern in expert_patterns[:2]:  # é™åˆ¶æœ€å¤š2ä¸ªæ¨¡å¼
            insights = get_expert_insights(pattern)
            standards = get_industry_standards(pattern)
            
            if insights:
                domain_context += f"- **{pattern}ä¸“å®¶æ´å¯Ÿ**:\n"
                for insight in insights[:2]:  # é™åˆ¶æœ€å¤š2æ¡æ´å¯Ÿ
                    domain_context += f"  - {insight}\n"
            
            if standards:
                domain_context += f"- **{pattern}è¡Œä¸šæ ‡å‡†**:\n"
                for standard in standards[:2]:  # é™åˆ¶æœ€å¤š2æ¡æ ‡å‡†
                    domain_context += f"  - {standard}\n"
        
        # æ·»åŠ è¡Œä¸šæœ€ä½³å®è·µ
        domain_context += "\n## è¡Œä¸šæœ€ä½³å®è·µ\n"
        best_practices = get_best_practices("å¾®æœåŠ¡æ¶æ„")
        if best_practices:
            domain_context += "- **å¾®æœåŠ¡æ¶æ„æœ€ä½³å®è·µ**:\n"
            for category, practices in list(best_practices.items())[:2]:  # é™åˆ¶æœ€å¤š2ä¸ªç±»åˆ«
                domain_context += f"  - {category}: {', '.join(practices[:2])}\n"
        
        return domain_context

    def _extract_log_level(self, content: str) -> str:
        """æå–æ—¥å¿—çº§åˆ«"""
        import re
        levels = ['FATAL', 'ERROR', 'WARN', 'WARNING', 'INFO', 'INFORMATION', 'DEBUG']
        for level in levels:
            if re.search(r'\b' + level + r'\b', content, re.IGNORECASE):
                return level
        return "UNKNOWN"

    def _extract_error_codes(self, content: str) -> List[str]:
        """æå–é”™è¯¯ç """
        import re
        # åŒ¹é…å¤§å†™å­—æ¯+æ•°å­—+ä¸‹åˆ’çº¿çš„æ¨¡å¼
        pattern = r'\b[A-Z][A-Z0-9_]{2,}\b'
        matches = re.findall(pattern, content)
        # è¿‡æ»¤æ‰å¸¸è§çš„éé”™è¯¯ç è¯æ±‡
        exclude_words = {'HTTP', 'URL', 'API', 'JSON', 'XML', 'SQL', 'TCP', 'UDP', 'IP', 'DNS'}
        return [match for match in matches if match not in exclude_words]

    def _extract_services(self, content: str) -> List[str]:
        """æå–æœåŠ¡åç§°"""
        import re
        # åŒ¹é…ä»¥Serviceç»“å°¾çš„è¯æ±‡
        pattern = r'\b[A-Za-z][A-Za-z0-9]*Service\b'
        return re.findall(pattern, content)

        # æ‰§è¡ŒæŸ¥è¯¢

    def query(self, query: str) -> Dict:
        log_results = self.retrieve_logs(query)
        response = self.generate_response(query, log_results)  # ç”Ÿæˆå“åº”

        return {
            "response": response,
            "retrieval_stats": len(log_results)
        }

    # ç¤ºä¾‹ä½¿ç”¨


if __name__ == "__main__":
    # åˆå§‹åŒ–ç³»ç»Ÿ
    system = TopKLogSystem(
        log_path="./data/log",
        llm="deepseek-r1:7b",
        embedding_model="bge-large:latest"
    )

    # æ‰§è¡ŒæŸ¥è¯¢
    query = "å¦‚ä½•è§£å†³æ•°æ®åº“è¿æ¥æ± è€—å°½çš„é—®é¢˜ï¼Ÿ"
    result = system.query(query)

    print("æŸ¥è¯¢:", query)
    print("å“åº”:", result["response"])
    print("æ£€ç´¢ç»Ÿè®¡:", result["retrieval_stats"])
