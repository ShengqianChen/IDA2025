import time
import threading
import json
import re
from typing import Dict, Any, Optional
from django.core.cache import cache
import hashlib
from .models import APIKey, RateLimit, ConversationSession
from django.conf import settings

# å…¨å±€é…ç½®
# API_KEY_LENGTH = 32
# TOKEN_EXPIRY_SECONDS = 3600
# RATE_LIMIT_MAX = 5  # æ¯åˆ†é’Ÿæœ€å¤§è¯·æ±‚æ•°
# RATE_LIMIT_INTERVAL = 60

# çº¿ç¨‹é”ç”¨äºé€Ÿç‡é™åˆ¶
rate_lock = threading.Lock()

def json_to_markdown(json_response: str) -> str:
    """
    å°†JSONæ ¼å¼çš„AIå“åº”è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œé€‚é…å‰ç«¯æ˜¾ç¤º
    """
    try:
        # æ¸…ç†Markdownä»£ç å—æ ‡è®°
        cleaned_response = json_response.strip()
        if cleaned_response.startswith('```json'):
            cleaned_response = cleaned_response[7:]  # ç§»é™¤ ```json
        if cleaned_response.startswith('```'):
            cleaned_response = cleaned_response[3:]   # ç§»é™¤ ```
        if cleaned_response.endswith('```'):
            cleaned_response = cleaned_response[:-3]  # ç§»é™¤ç»“å°¾çš„ ```
        
        # æ¸…ç†HTMLæ ‡ç­¾ï¼Œæå–çº¯æ–‡æœ¬
        cleaned_response = re.sub(r'<[^>]+>', '', cleaned_response)
        
        # å°è¯•è§£æJSON
        data = json.loads(cleaned_response.strip())
        
        # æ„å»ºMarkdownæ ¼å¼çš„åˆ†ææŠ¥å‘Š
        markdown_content = "# ğŸ” æ•…éšœåˆ†ææŠ¥å‘Š\n\n"
        
        # æ•…éšœæ‘˜è¦
        fault_summary = data.get("fault_summary", {})
        markdown_content += "## ğŸ“‹ æ•…éšœæ‘˜è¦\n\n"
        markdown_content += f"**ä¸¥é‡ç¨‹åº¦**: {fault_summary.get('severity', 'UNKNOWN')}\n\n"
        markdown_content += f"**æ•…éšœåˆ†ç±»**: {fault_summary.get('category', 'UNKNOWN')}\n\n"
        markdown_content += f"**æ•…éšœæè¿°**: {fault_summary.get('description', 'æ— æè¿°')}\n\n"
        
        affected_services = fault_summary.get('affected_services', [])
        if affected_services:
            markdown_content += f"**å—å½±å“æœåŠ¡**: {', '.join(affected_services)}\n\n"
        
        error_codes = fault_summary.get('error_codes', [])
        if error_codes:
            markdown_content += f"**é”™è¯¯ç **: {', '.join(error_codes)}\n\n"
        
        # æ ¹å› åˆ†æ
        root_cause = data.get("root_cause_analysis", {})
        markdown_content += "## ğŸ” æ ¹å› åˆ†æ\n\n"
        markdown_content += f"**ä¸»è¦åŸå› **: {root_cause.get('primary_cause', 'æœªè¯†åˆ«')}\n\n"
        
        contributing_factors = root_cause.get('contributing_factors', [])
        if contributing_factors:
            markdown_content += "**å½±å“å› ç´ **:\n"
            for factor in contributing_factors:
                markdown_content += f"- {factor}\n"
            markdown_content += "\n"
        
        markdown_content += f"**ç½®ä¿¡åº¦**: {root_cause.get('confidence_level', 'UNKNOWN')}\n\n"
        markdown_content += f"**åˆ†ææ¨ç†**: {root_cause.get('reasoning', 'æ— æ¨ç†è¿‡ç¨‹')}\n\n"
        
        # è§£å†³æ–¹æ¡ˆ
        solutions = data.get("solutions", {})
        markdown_content += "## ğŸ› ï¸ è§£å†³æ–¹æ¡ˆ\n\n"
        
        # ç«‹å³è¡ŒåŠ¨
        immediate_actions = solutions.get("immediate_actions", [])
        if immediate_actions:
            markdown_content += "### ğŸš¨ ç«‹å³è¡ŒåŠ¨\n\n"
            for action in immediate_actions:
                priority = action.get('priority', 'UNKNOWN')
                priority_emoji = {"HIGH": "ğŸ”´", "MEDIUM": "ğŸŸ¡", "LOW": "ğŸŸ¢"}.get(priority, "âšª")
                markdown_content += f"{priority_emoji} **{priority}**: {action.get('action', 'æ— è¡ŒåŠ¨')}\n\n"
        
        # é•¿æœŸä¿®å¤
        long_term_fixes = solutions.get("long_term_fixes", [])
        if long_term_fixes:
            markdown_content += "### ğŸ”§ é•¿æœŸä¿®å¤\n\n"
            for fix in long_term_fixes:
                priority = fix.get('priority', 'UNKNOWN')
                priority_emoji = {"HIGH": "ğŸ”´", "MEDIUM": "ğŸŸ¡", "LOW": "ğŸŸ¢"}.get(priority, "âšª")
                markdown_content += f"{priority_emoji} **{priority}**: {fix.get('action', 'æ— è¡ŒåŠ¨')}\n\n"
        
        # é¢„é˜²æªæ–½
        prevention_measures = solutions.get("prevention_measures", [])
        if prevention_measures:
            markdown_content += "### ğŸ›¡ï¸ é¢„é˜²æªæ–½\n\n"
            for measure in prevention_measures:
                priority = measure.get('priority', 'UNKNOWN')
                priority_emoji = {"HIGH": "ğŸ”´", "MEDIUM": "ğŸŸ¡", "LOW": "ğŸŸ¢"}.get(priority, "âšª")
                markdown_content += f"{priority_emoji} **{priority}**: {measure.get('action', 'æ— è¡ŒåŠ¨')}\n\n"
        
        # ç›‘æ§å»ºè®®
        monitoring_recommendations = data.get("monitoring_recommendations", [])
        if monitoring_recommendations:
            markdown_content += "## ğŸ“Š ç›‘æ§å»ºè®®\n\n"
            for i, recommendation in enumerate(monitoring_recommendations, 1):
                markdown_content += f"{i}. {recommendation}\n"
            markdown_content += "\n"
        
        return markdown_content
        
    except json.JSONDecodeError:
        # å¦‚æœä¸æ˜¯æœ‰æ•ˆçš„JSONï¼Œè¿”å›åŸå§‹å“åº”
        return json_response
    except Exception as e:
        # å…¶ä»–é”™è¯¯ï¼Œè¿”å›åŸå§‹å“åº”
        return json_response

def deepseek_r1_api_call(prompt: str, session_context: str = "", conversation_type: str = "fault_analysis") -> str:
    """æ™ºèƒ½ DeepSeek-R1 API è°ƒç”¨å‡½æ•°"""
    from topklogsystem import TopKLogSystem, ConversationType
    
    system = TopKLogSystem(
        log_path="./data/log",
        llm="deepseek-r1:7b",
        embedding_model="bge-large:latest"
    )

    query = prompt
    
    # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
    context = {
        'context': session_context,
        'logs': []  # è¿™é‡Œå¯ä»¥æ·»åŠ æ£€ç´¢åˆ°çš„æ—¥å¿—ä¿¡æ¯
    }
    
    # ç”Ÿæˆå“åº”
    result = system.generate_response(query, context)
    time.sleep(0.5)

    # è·å–åŸå§‹å“åº”
    raw_response = result
    print(f"ğŸ” åŸå§‹å“åº”é•¿åº¦: {len(raw_response)} å­—ç¬¦")
    print(f"ğŸ” å¯¹è¯ç±»å‹: {conversation_type}")
    print(f"ğŸ” åŸå§‹å“åº”å‰200å­—ç¬¦: {raw_response[:200]}...")
    
    # æ ¹æ®å¯¹è¯ç±»å‹è¿›è¡Œæ™ºèƒ½å¤„ç†
    processed_response = process_response_by_type(raw_response, conversation_type)
    print(f"âœ… å¤„ç†åå“åº”é•¿åº¦: {len(processed_response)} å­—ç¬¦")
    
    return processed_response

def process_response_by_type(response: str, conversation_type: str) -> str:
    """
    æ ¹æ®å¯¹è¯ç±»å‹æ™ºèƒ½å¤„ç†å“åº”
    
    Args:
        response: åŸå§‹å“åº”
        conversation_type: å¯¹è¯ç±»å‹
        
    Returns:
        str: å¤„ç†åçš„å“åº”
    """
    try:
        # ç°åœ¨æ‰€æœ‰å¯¹è¯ç±»å‹éƒ½è¿”å›Markdownæ ¼å¼ï¼Œç›´æ¥è¿”å›
        return response
    except Exception as e:
        print(f"âš ï¸ å“åº”å¤„ç†å‡ºé”™: {e}")
        # å‡ºé”™æ—¶è¿”å›åŸå§‹å“åº”
        return response

def assess_response_quality(response: str, conversation_type: str) -> dict:
    """
    è¯„ä¼°å“åº”è´¨é‡
    
    Args:
        response: å“åº”å†…å®¹
        conversation_type: å¯¹è¯ç±»å‹
        
    Returns:
        dict: è´¨é‡æŒ‡æ ‡
    """
    quality_metrics = {
        'length': len(response),
        'has_structure': False,
        'has_keywords': False,
        'format_correct': False,
        'completeness_score': 0,
        'relevance_score': 0
    }
    
    if not response or len(response.strip()) == 0:
        return quality_metrics
    
    # æ£€æŸ¥Markdownç»“æ„
    quality_metrics['has_structure'] = any(marker in response for marker in ['#', '##', '###', '-', '*'])
    quality_metrics['format_correct'] = quality_metrics['has_structure']
    
    # æ£€æŸ¥å…³é”®è¯ï¼ˆæ›´å®½æ¾çš„å…³é”®è¯åŒ¹é…ï¼‰
    keywords = [
        "é”™è¯¯", "æ•…éšœ", "å¼‚å¸¸", "å¤±è´¥", "æœåŠ¡", "æ•°æ®åº“", "ç½‘ç»œ", "è¿æ¥", "è¶…æ—¶", "å†…å­˜", "CPU",
        "error", "fatal", "exception", "service", "database", "network", "timeout", "memory",
        "é—®é¢˜", "è§£å†³", "åˆ†æ", "åŸå› ", "å»ºè®®", "ç›‘æ§", "é¢„é˜²", "ä¿®å¤", "ä¼˜åŒ–"
    ]
    keyword_count = sum(response.lower().count(keyword) for keyword in keywords)
    quality_metrics['has_keywords'] = keyword_count > 0
    
    # è®¡ç®—å®Œæ•´æ€§å¾—åˆ†ï¼ˆåŸºäºMarkdownç»“æ„ï¼‰
    structure_elements = ["#", "##", "###", "-", "*"]
    present_elements = sum(1 for element in structure_elements if element in response)
    quality_metrics['completeness_score'] = min(present_elements / 2, 1.0)  # è‡³å°‘éœ€è¦2ä¸ªç»“æ„å…ƒç´ 
    
    # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†ï¼ˆåŸºäºå…³é”®è¯å¯†åº¦ï¼Œä½†æ›´å®½æ¾ï¼‰
    if len(response) > 0:
        keyword_density = keyword_count / len(response)
        # å°†å¯†åº¦è½¬æ¢ä¸º0-1ä¹‹é—´çš„åˆ†æ•°ï¼Œå¯†åº¦0.01ä»¥ä¸Šå°±è®¤ä¸ºç›¸å…³æ€§è¾ƒé«˜
        quality_metrics['relevance_score'] = min(keyword_density * 100, 1.0)
    else:
        quality_metrics['relevance_score'] = 0
    
    return quality_metrics

def optimize_response(response: str, conversation_type: str) -> str:
    """
    ä¼˜åŒ–å“åº”å†…å®¹
    
    Args:
        response: åŸå§‹å“åº”
        conversation_type: å¯¹è¯ç±»å‹
        
    Returns:
        str: ä¼˜åŒ–åçš„å“åº”
    """
    try:
        if conversation_type == "fault_analysis":
            # æ•…éšœåˆ†æç±»å‹ï¼šç¡®ä¿JSONæ ¼å¼æ­£ç¡®
            return optimize_json_response(response)
        else:
            # å…¶ä»–ç±»å‹ï¼šä¼˜åŒ–Markdownæ ¼å¼
            return optimize_markdown_response(response)
    except Exception as e:
        print(f"âš ï¸ å“åº”ä¼˜åŒ–å‡ºé”™: {e}")
        return response

def optimize_json_response(response: str) -> str:
    """ä¼˜åŒ–JSONå“åº”"""
    import json
    import re
    
    try:
        # æ¸…ç†å“åº”
        cleaned_response = re.sub(r'<[^>]+>', '', response)  # ç§»é™¤HTMLæ ‡ç­¾
        cleaned_response = re.sub(r'```json\s*', '', cleaned_response)  # ç§»é™¤markdownä»£ç å—æ ‡è®°
        cleaned_response = re.sub(r'```\s*$', '', cleaned_response)
        
        # å°è¯•è§£æJSON
        json_data = json.loads(cleaned_response)
        
        # ç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µå­˜åœ¨
        required_fields = {
            "fault_summary": {
                "severity": "MEDIUM",
                "category": "SYSTEM_RESOURCE",
                "description": "ç³»ç»Ÿæ•…éšœ",
                "affected_services": [],
                "error_codes": [],
                "impact_scope": "å½±å“èŒƒå›´æœªçŸ¥"
            },
            "root_cause_analysis": {
                "primary_cause": "åŸå› æœªçŸ¥",
                "contributing_factors": [],
                "confidence_level": "MEDIUM",
                "reasoning": "åˆ†ææ¨ç†",
                "evidence": []
            },
            "solutions": {
                "immediate_actions": [],
                "long_term_fixes": [],
                "prevention_measures": []
            },
            "monitoring_recommendations": []
        }
        
        # å¡«å……ç¼ºå¤±å­—æ®µ
        for field, default_value in required_fields.items():
            if field not in json_data:
                json_data[field] = default_value
            elif isinstance(default_value, dict):
                for sub_field, sub_default in default_value.items():
                    if sub_field not in json_data[field]:
                        json_data[field][sub_field] = sub_default
        
        return json.dumps(json_data, ensure_ascii=False, indent=2)
        
    except json.JSONDecodeError:
        # å¦‚æœä¸æ˜¯æœ‰æ•ˆJSONï¼Œè¿”å›åŸå§‹å“åº”
        return response

def optimize_markdown_response(response: str) -> str:
    """ä¼˜åŒ–Markdownå“åº”"""
    import re
    
    # æ¸…ç†HTMLæ ‡ç­¾
    cleaned_response = re.sub(r'<[^>]+>', '', response)
    
    # ç¡®ä¿æ ‡é¢˜æ ¼å¼æ­£ç¡®
    lines = cleaned_response.split('\n')
    optimized_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # ç¡®ä¿æ ‡é¢˜æœ‰æ­£ç¡®çš„æ ¼å¼
        if line.startswith('#'):
            # æ ‡é¢˜è¡Œ
            optimized_lines.append(line)
        elif line.startswith('-') or line.startswith('*'):
            # åˆ—è¡¨é¡¹
            optimized_lines.append(line)
        elif line.startswith('```'):
            # ä»£ç å—
            optimized_lines.append(line)
        else:
            # æ™®é€šæ–‡æœ¬
            optimized_lines.append(line)
    
    return '\n'.join(optimized_lines)

def create_api_key(user: str) -> str:
    """åˆ›å»º API Key å¹¶ä¿å­˜åˆ°æ•°æ®åº“"""
    key = APIKey.generate_key()
    expiry = time.time() + settings.TOKEN_EXPIRY_SECONDS
    
    api_key = APIKey.objects.create(
        key=key,
        user=user,
        expiry_time=expiry
    )
    
    # åˆ›å»ºå¯¹åº”çš„é€Ÿç‡é™åˆ¶è®°å½•
    RateLimit.objects.create(
        api_key=api_key,
        reset_time=time.time() + settings.RATE_LIMIT_INTERVAL
    )
    
    return key

def validate_api_key(key_str: str) -> bool:
    """éªŒè¯ API Key æ˜¯å¦å­˜åœ¨ä¸”æœªè¿‡æœŸ"""
    try:
        api_key = APIKey.objects.get(key=key_str)
        if api_key.is_valid():
            return True
        else:
            api_key.delete()  # åˆ é™¤è¿‡æœŸkey
            return False
    except APIKey.DoesNotExist:
        return False

def check_rate_limit(key_str: str) -> bool:
    """æ£€æŸ¥ API Key çš„è¯·æ±‚é¢‘ç‡æ˜¯å¦è¶…è¿‡é™åˆ¶"""
    with rate_lock:
        try:
            # api_key = APIKey.objects.get(key=key_str)
            # rate_limit = RateLimit.objects.get(api_key=api_key)
            rate_limit = RateLimit.objects.select_related('api_key').get(api_key__key=key_str)
            
            current_time = time.time()
            if current_time > rate_limit.reset_time:
                rate_limit.count = 1
                rate_limit.reset_time = current_time + settings.RATE_LIMIT_INTERVAL
                rate_limit.save()
                return True
            elif rate_limit.count < settings.RATE_LIMIT_MAX:
                rate_limit.count += 1
                rate_limit.save()
                return True
            else:
                return False
        except RateLimit.DoesNotExist:
            # å¦‚æœé€Ÿç‡é™åˆ¶è®°å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
            try:
                current_time = time.time()
                api_key = APIKey.objects.get(key=key_str)
                RateLimit.objects.create(
                    api_key=api_key,
                    count=1,
                    reset_time=current_time + settings.RATE_LIMIT_INTERVAL
                )
                return True
            except APIKey.DoesNotExist:
                return False

# def get_or_create_session(session_id: str, user: APIKey) -> ConversationSession:
    # """è·å–æˆ–åˆ›å»ºä¼šè¯ï¼Œå…³è”å½“å‰ç”¨æˆ·ï¼ˆé€šè¿‡API Keyï¼‰"""
    # session, created = ConversationSession.objects.get_or_create(
        # session_id=session_id,
        # user=user,  # ç»‘å®šç”¨æˆ·
        # defaults={'context': ''}
    # )
    # return session

def get_or_create_session(session_id: str, user: APIKey) -> ConversationSession:
    """
    è·å–æˆ–åˆ›å»ºç”¨æˆ·çš„ä¸“å±ä¼šè¯ï¼š
    - è‹¥ç”¨æˆ·+session_idå·²å­˜åœ¨ â†’ åŠ è½½æ—§ä¼šè¯ï¼ˆä¿ç•™å†å²ï¼‰
    - è‹¥ä¸å­˜åœ¨ â†’ åˆ›å»ºæ–°ä¼šè¯ï¼ˆç©ºå†å²ï¼‰
    """
    session, created = ConversationSession.objects.get_or_create(
        session_id=session_id,  # åŒ¹é…ä¼šè¯ID
        user=user,              # åŒ¹é…å½“å‰ç”¨æˆ·ï¼ˆå…³é”®ï¼é¿å…è·¨ç”¨æˆ·ä¼šè¯å†²çªï¼‰
        defaults={'context': ''}
    )
    # è°ƒè¯•æ—¥å¿—ï¼šç¡®è®¤æ˜¯å¦åˆ›å»ºæ–°ä¼šè¯ï¼ˆcreated=True è¡¨ç¤ºæ–°ä¼šè¯ï¼‰
    import logging
    logger = logging.getLogger(__name__)
    logger.info(f"ä¼šè¯ {session_id}ï¼ˆç”¨æˆ·ï¼š{user.user}ï¼‰{'åˆ›å»ºæ–°ä¼šè¯' if created else 'åŠ è½½æ—§ä¼šè¯'}")
    return session

def get_cached_reply(prompt: str, session_id: str, user: APIKey) -> str | None:
    """ç¼“å­˜é”®åŒ…å« session_id å’Œ userï¼Œé¿å…è·¨ä¼šè¯å†²çª"""
    cache_key = f"reply:{user.user}:{session_id}:{hash(prompt)}"
    return cache.get(cache_key)

def set_cached_reply(prompt: str, reply: str, session_id: str, user: APIKey, timeout=3600):
    cache_key = f"reply:{user.user}:{session_id}:{hash(prompt)}"
    cache.set(cache_key, reply, timeout)


def generate_cache_key(original_key: str) -> str:
    """
    ç”Ÿæˆå®‰å…¨çš„ç¼“å­˜é”®ã€‚
    å¯¹åŸå§‹å­—ç¬¦ä¸²è¿›è¡Œå“ˆå¸Œå¤„ç†ï¼Œç¡®ä¿é”®é•¿åº¦å›ºå®šä¸”ä»…åŒ…å«å®‰å…¨å­—ç¬¦ã€‚
    """
    # ä½¿ç”¨SHA256å“ˆå¸Œå‡½æ•°ç”Ÿæˆå›ºå®šé•¿åº¦çš„é”®ï¼ˆ64ä½åå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼‰
    hash_obj = hashlib.sha256(original_key.encode('utf-8'))
    return hash_obj.hexdigest()
