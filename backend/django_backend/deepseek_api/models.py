from django.db import models
from django.db.models import F
import string
import random
import time
import logging
logger = logging.getLogger(__name__)

from django.db.models import indexes

class APIKey(models.Model):
    key = models.CharField(max_length=32, unique=True)
    user = models.CharField(max_length=100)
    created_at = models.DateTimeField(auto_now_add=True)
    expiry_time = models.IntegerField()  # è¿‡æœŸæ—¶é—´æˆ³
    
    @classmethod
    def generate_key(cls, length=32):
        """ç”Ÿæˆéšæœº API Key"""
        characters = string.ascii_letters + string.digits
        return ''.join(random.choice(characters) for _ in range(length))
    
    def is_valid(self):
        """æ£€æŸ¥ API Key æ˜¯å¦æœªè¿‡æœŸ"""
        return time.time() < self.expiry_time
    
    def __str__(self):
        return f"{self.user} - {self.key}"


class RateLimit(models.Model):
    api_key = models.ForeignKey(APIKey, on_delete=models.CASCADE,
                                db_index=True, to_field='key', related_name='rate_limits')
    count = models.IntegerField(default=0)
    reset_time = models.IntegerField()  # é‡ç½®æ—¶é—´æˆ³

    class Meta:
        indexes = [
            models.Index(fields=['api_key', 'reset_time'])
        ]
    
    def should_limit(self, max_requests, interval):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥é™åˆ¶è¯·æ±‚"""
        current_time = time.time()
        if current_time > self.reset_time:
            self.count = 0
            self.reset_time = current_time + interval
            self.save()
            return False
        return self.count >= max_requests


class ConversationSession(models.Model):
    session_id = models.CharField(max_length=100)
    # æ­£ç¡®çš„å¤–é”®å®šä¹‰ï¼šå…³è” APIKey çš„ idï¼ˆé»˜è®¤ï¼‰
    user = models.ForeignKey(
        APIKey, 
        on_delete=models.CASCADE, 
        related_name='sessions'
    )
    context = models.TextField(blank=True)
    # æ–°å¢å­—æ®µï¼šå¤šè½®å¯¹è¯åŠŸèƒ½æ”¯æŒ
    context_summary = models.TextField(blank=True, help_text="ä¸Šä¸‹æ–‡æ‘˜è¦ï¼Œç”¨äºå‹ç¼©å†å²å¯¹è¯")
    recent_context = models.TextField(blank=True, help_text="æœ€è¿‘Nè½®å¯¹è¯ï¼Œä¿æŒå®Œæ•´æ ¼å¼")
    max_context_length = models.IntegerField(default=4000, help_text="æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶")
    conversation_type = models.CharField(
        max_length=50, 
        default='fault_analysis',
        choices=[
            ('fault_analysis', 'æ•…éšœåˆ†æ'),
            ('general_question', 'ä¸€èˆ¬é—®é¢˜'),
            ('follow_up', 'è·Ÿè¿›é—®é¢˜'),
            ('explanation', 'è§£é‡Šè¯·æ±‚'),
            ('prevention', 'é¢„é˜²æªæ–½'),
            ('dependency', 'ä¾èµ–å…³ç³»'),
        ],
        help_text="å½“å‰å¯¹è¯ç±»å‹"
    )
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    class Meta:
        unique_together = ('session_id', 'user')  # ç¡®ä¿ç”¨æˆ·+ä¼šè¯IDå”¯ä¸€
    
    def update_context(self, user_input, bot_reply):
        """åŸå­æ›´æ–°ä¸Šä¸‹æ–‡ï¼Œé¿å…å¹¶å‘è¦†ç›–"""
        new_entry = f"ç”¨æˆ·ï¼š{user_input}\nå›å¤ï¼š{bot_reply}\n"
        # æ•°æ®åº“å±‚é¢æ‹¼æ¥ï¼Œè€Œéå†…å­˜ä¸­
        ConversationSession.objects.filter(
            pk=self.pk,  # ç²¾ç¡®åŒ¹é…å½“å‰ä¼šè¯
            user=self.user  # ç¡®ä¿ç”¨æˆ·ä¸€è‡´
        ).update(context=F('context') + new_entry)
        # åˆ·æ–°å®ä¾‹ï¼Œè·å–æ›´æ–°åçš„å€¼
        self.refresh_from_db()

        # import logging
        # logger = logging.getLogger(__name__)
        # logger.info(f"æ›´æ–°ä¼šè¯ {self.session_id}ï¼ˆç”¨æˆ·ï¼š{self.user.key}ï¼‰ï¼š{new_entry}")
    
    def clear_context(self):
        """æ¸…ç©ºå¯¹è¯ä¸Šä¸‹æ–‡"""
        self.context = ""
        self.context_summary = ""
        self.recent_context = ""
        self.save()
    
    def compress_context_if_needed(self, max_length: int = None):
        """æ£€æŸ¥å¹¶å‹ç¼©ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        if max_length is None:
            max_length = self.max_context_length
        
        if len(self.context) <= max_length:
            return False
        
        # éœ€è¦å‹ç¼©ä¸Šä¸‹æ–‡
        self._compress_context()
        return True
    
    def _compress_context(self):
        """æ™ºèƒ½å‹ç¼©ä¸Šä¸‹æ–‡ï¼Œä¿ç•™æœ€è¿‘å¯¹è¯å¹¶ç”Ÿæˆæ‘˜è¦"""
        import re
        
        # åˆ†å‰²å¯¹è¯è½®æ¬¡
        conversations = self.context.split('ç”¨æˆ·ï¼š')
        if len(conversations) <= 3:
            return
        
        # æ™ºèƒ½å†³å®šä¿ç•™è½®æ•°ï¼ˆåŸºäºä¸Šä¸‹æ–‡é•¿åº¦ï¼‰
        total_length = len(self.context)
        if total_length > self.max_context_length * 2:
            # å¦‚æœä¸Šä¸‹æ–‡å¾ˆé•¿ï¼Œåªä¿ç•™æœ€è¿‘2è½®
            keep_rounds = 2
        else:
            # å¦åˆ™ä¿ç•™æœ€è¿‘3è½®
            keep_rounds = 3
        
        # ä¿ç•™æœ€è¿‘å¯¹è¯
        recent_conversations = conversations[-keep_rounds:]
        self.recent_context = 'ç”¨æˆ·ï¼š'.join(recent_conversations)
        
        # å‹ç¼©æ—©æœŸå¯¹è¯ä¸ºæ‘˜è¦
        early_conversations = conversations[:-keep_rounds]
        self.context_summary = self._generate_context_summary(early_conversations)
        
        # æ›´æ–°å®Œæ•´ä¸Šä¸‹æ–‡
        compressed_context = f"{self.context_summary}\n\n{self.recent_context}"
        
        # å¦‚æœå‹ç¼©åä»ç„¶å¤ªé•¿ï¼Œè¿›è¡ŒäºŒæ¬¡å‹ç¼©
        if len(compressed_context) > self.max_context_length:
            compressed_context = self._secondary_compress(compressed_context)
        
        self.context = compressed_context
        self.save()
    
    def _generate_context_summary(self, conversations):
        """æ™ºèƒ½ç”Ÿæˆä¸Šä¸‹æ–‡æ‘˜è¦"""
        import re
        
        # æå–å…³é”®ä¿¡æ¯
        key_info = {
            'error_codes': set(),
            'services': set(),
            'keywords': set(),
            'time_patterns': set(),
            'user_patterns': set(),
            'topics': set()
        }
        
        for conv in conversations:
            # æå–é”™è¯¯ç 
            error_patterns = [
                r'ERROR\s+(\d+)',
                r'FATAL\s+(\d+)',
                r'Exception:\s*(\w+)',
                r'é”™è¯¯ç [ï¼š:]\s*(\w+)'
            ]
            for pattern in error_patterns:
                matches = re.findall(pattern, conv, re.IGNORECASE)
                key_info['error_codes'].update(matches)
            
            # æå–æœåŠ¡å
            service_patterns = [
                r'æœåŠ¡[ï¼š:]\s*([^\s\n]+)',
                r'Service[ï¼š:]\s*([^\s\n]+)',
                r'æ¨¡å—[ï¼š:]\s*([^\s\n]+)',
                r'ç»„ä»¶[ï¼š:]\s*([^\s\n]+)'
            ]
            for pattern in service_patterns:
                matches = re.findall(pattern, conv, re.IGNORECASE)
                key_info['services'].update(matches)
            
            # æå–æ—¶é—´æ¨¡å¼
            time_patterns = [
                r'\d{4}-\d{2}-\d{2}',
                r'\d{2}:\d{2}:\d{2}',
                r'(\d+)\s*åˆ†é’Ÿå‰',
                r'(\d+)\s*å°æ—¶å‰'
            ]
            for pattern in time_patterns:
                matches = re.findall(pattern, conv)
                key_info['time_patterns'].update(matches)
            
            # æå–ç”¨æˆ·æ¨¡å¼
            user_patterns = [
                r'ç”¨æˆ·[ï¼š:]\s*([^\s\n]+)',
                r'User[ï¼š:]\s*([^\s\n]+)',
                r'ç”¨æˆ·ID[ï¼š:]\s*([^\s\n]+)'
            ]
            for pattern in user_patterns:
                matches = re.findall(pattern, conv, re.IGNORECASE)
                key_info['user_patterns'].update(matches)
            
            # æå–å…³é”®è¯å’Œä¸»é¢˜
            important_words = [
                "é”™è¯¯", "æ•…éšœ", "å¼‚å¸¸", "å¤±è´¥", "æœåŠ¡", "æ•°æ®åº“", "ç½‘ç»œ",
                "è¿æ¥", "è¶…æ—¶", "å†…å­˜", "CPU", "ç£ç›˜", "æ—¥å¿—", "ç›‘æ§",
                "å‘Šè­¦", "æ¢å¤", "é‡å¯", "éƒ¨ç½²", "å‘å¸ƒ", "å›æ»š"
            ]
            for word in important_words:
                if word in conv:
                    key_info['keywords'].add(word)
            
            # æå–ä¸»é¢˜ï¼ˆåŸºäºå¯¹è¯å†…å®¹ï¼‰
            if "æ•°æ®åº“" in conv:
                key_info['topics'].add("æ•°æ®åº“é—®é¢˜")
            if "ç½‘ç»œ" in conv:
                key_info['topics'].add("ç½‘ç»œé—®é¢˜")
            if "å†…å­˜" in conv or "CPU" in conv:
                key_info['topics'].add("æ€§èƒ½é—®é¢˜")
            if "éƒ¨ç½²" in conv or "å‘å¸ƒ" in conv:
                key_info['topics'].add("éƒ¨ç½²é—®é¢˜")
        
        # æ„å»ºæ™ºèƒ½æ‘˜è¦
        summary_parts = ["## ğŸ“‹ å†å²å¯¹è¯æ‘˜è¦"]
        
        # ä¸»é¢˜æ‘˜è¦
        if key_info['topics']:
            summary_parts.append(f"**è®¨è®ºä¸»é¢˜**: {', '.join(key_info['topics'])}")
        
        # é”™è¯¯ç æ‘˜è¦
        if key_info['error_codes']:
            error_codes_str = ', '.join(list(key_info['error_codes'])[:5])  # æœ€å¤šæ˜¾ç¤º5ä¸ª
            if len(key_info['error_codes']) > 5:
                error_codes_str += f" ç­‰{len(key_info['error_codes'])}ä¸ªé”™è¯¯ç "
            summary_parts.append(f"**æ¶‰åŠé”™è¯¯ç **: {error_codes_str}")
        
        # æœåŠ¡æ‘˜è¦
        if key_info['services']:
            services_str = ', '.join(list(key_info['services'])[:3])  # æœ€å¤šæ˜¾ç¤º3ä¸ª
            if len(key_info['services']) > 3:
                services_str += f" ç­‰{len(key_info['services'])}ä¸ªæœåŠ¡"
            summary_parts.append(f"**æ¶‰åŠæœåŠ¡**: {services_str}")
        
        # å…³é”®è¯æ‘˜è¦
        if key_info['keywords']:
            keywords_str = ', '.join(list(key_info['keywords'])[:8])  # æœ€å¤šæ˜¾ç¤º8ä¸ª
            summary_parts.append(f"**å…³é”®è¯**: {keywords_str}")
        
        # æ—¶é—´æ¨¡å¼æ‘˜è¦
        if key_info['time_patterns']:
            summary_parts.append(f"**æ—¶é—´èŒƒå›´**: åŒ…å«{len(key_info['time_patterns'])}ä¸ªæ—¶é—´ç‚¹")
        
        # ç”¨æˆ·æ¨¡å¼æ‘˜è¦
        if key_info['user_patterns']:
            summary_parts.append(f"**æ¶‰åŠç”¨æˆ·**: {len(key_info['user_patterns'])}ä¸ªç”¨æˆ·")
        
        # æ·»åŠ å¯¹è¯è½®æ•°ç»Ÿè®¡
        summary_parts.append(f"**å¯¹è¯è½®æ•°**: {len(conversations)}è½®")
        
        return '\n'.join(summary_parts)
    
    def _secondary_compress(self, context: str) -> str:
        """äºŒæ¬¡å‹ç¼©ï¼Œè¿›ä¸€æ­¥å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦"""
        import re
        
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        context = re.sub(r'\n\s*\n\s*\n', '\n\n', context)
        
        # å‹ç¼©é•¿å¥å­ï¼ˆä¿ç•™å…³é”®ä¿¡æ¯ï¼‰
        lines = context.split('\n')
        compressed_lines = []
        
        for line in lines:
            if len(line) > 200:
                # é•¿å¥å­å‹ç¼©ï¼šä¿ç•™å‰100å­—ç¬¦å’Œå50å­—ç¬¦
                compressed_line = line[:100] + "..." + line[-50:]
                compressed_lines.append(compressed_line)
            else:
                compressed_lines.append(line)
        
        # é‡æ–°ç»„åˆ
        compressed_context = '\n'.join(compressed_lines)
        
        # å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œåªä¿ç•™æœ€è¿‘çš„å†…å®¹
        if len(compressed_context) > self.max_context_length:
            # æŒ‰æ®µè½åˆ†å‰²ï¼Œä¿ç•™æœ€åå‡ ä¸ªæ®µè½
            paragraphs = compressed_context.split('\n\n')
            keep_paragraphs = []
            current_length = 0
            
            # ä»åå¾€å‰æ·»åŠ æ®µè½
            for paragraph in reversed(paragraphs):
                if current_length + len(paragraph) <= self.max_context_length * 0.8:
                    keep_paragraphs.insert(0, paragraph)
                    current_length += len(paragraph)
                else:
                    break
            
            compressed_context = '\n\n'.join(keep_paragraphs)
        
        return compressed_context
    
    def assess_context_quality(self) -> dict:
        """è¯„ä¼°ä¸Šä¸‹æ–‡è´¨é‡"""
        context = self.context
        
        # è®¡ç®—è´¨é‡æŒ‡æ ‡
        quality_metrics = {
            'length': len(context),
            'conversation_rounds': len(context.split('ç”¨æˆ·ï¼š')) - 1,
            'has_summary': bool(self.context_summary),
            'has_recent_context': bool(self.recent_context),
            'compression_ratio': 0,
            'information_density': 0,
            'relevance_score': 0
        }
        
        # è®¡ç®—å‹ç¼©æ¯”
        if self.context_summary and self.recent_context:
            original_length = len(self.context_summary) + len(self.recent_context)
            compressed_length = len(context)
            quality_metrics['compression_ratio'] = compressed_length / original_length if original_length > 0 else 1
        
        # è®¡ç®—ä¿¡æ¯å¯†åº¦ï¼ˆå…³é”®è¯å¯†åº¦ï¼‰
        import re
        keywords = ["é”™è¯¯", "æ•…éšœ", "å¼‚å¸¸", "å¤±è´¥", "æœåŠ¡", "æ•°æ®åº“", "ç½‘ç»œ", "è¿æ¥", "è¶…æ—¶"]
        keyword_count = sum(context.lower().count(keyword) for keyword in keywords)
        quality_metrics['information_density'] = keyword_count / len(context) if len(context) > 0 else 0
        
        # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†ï¼ˆåŸºäºé”™è¯¯ç å’ŒæœåŠ¡åï¼‰
        error_codes = len(re.findall(r'ERROR\s+(\d+)', context))
        services = len(re.findall(r'æœåŠ¡[ï¼š:]\s*([^\s\n]+)', context))
        quality_metrics['relevance_score'] = (error_codes + services) / len(context) if len(context) > 0 else 0
        
        return quality_metrics
    
    def optimize_context_length(self, target_length: int = None) -> bool:
        """ä¼˜åŒ–ä¸Šä¸‹æ–‡é•¿åº¦åˆ°ç›®æ ‡é•¿åº¦"""
        if target_length is None:
            target_length = self.max_context_length
        
        current_length = len(self.context)
        if current_length <= target_length:
            return False
        
        # è®¡ç®—éœ€è¦å‹ç¼©çš„æ¯”ä¾‹
        compression_ratio = target_length / current_length
        
        if compression_ratio < 0.5:
            # éœ€è¦å¤§å¹…å‹ç¼©ï¼Œä½¿ç”¨æ¿€è¿›ç­–ç•¥
            self._aggressive_compress(target_length)
        elif compression_ratio < 0.8:
            # éœ€è¦ä¸­ç­‰å‹ç¼©ï¼Œä½¿ç”¨æ ‡å‡†ç­–ç•¥
            self._compress_context()
        else:
            # éœ€è¦è½»å¾®å‹ç¼©ï¼Œä½¿ç”¨ä¿å®ˆç­–ç•¥
            self._conservative_compress(target_length)
        
        return True
    
    def _aggressive_compress(self, target_length: int):
        """æ¿€è¿›å‹ç¼©ç­–ç•¥"""
        # åªä¿ç•™æœ€è¿‘1è½®å¯¹è¯
        conversations = self.context.split('ç”¨æˆ·ï¼š')
        if len(conversations) > 1:
            recent_conversation = conversations[-1]
            self.recent_context = f"ç”¨æˆ·ï¼š{recent_conversation}"
            
            # ç”Ÿæˆé«˜åº¦å‹ç¼©çš„æ‘˜è¦
            early_conversations = conversations[:-1]
            self.context_summary = self._generate_compact_summary(early_conversations)
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            self.context = f"{self.context_summary}\n\n{self.recent_context}"
            
            # å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œè¿›è¡ŒäºŒæ¬¡å‹ç¼©
            if len(self.context) > target_length:
                self.context = self._secondary_compress(self.context)
            
            self.save()
    
    def _conservative_compress(self, target_length: int):
        """ä¿å®ˆå‹ç¼©ç­–ç•¥"""
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œå’Œé‡å¤å†…å®¹
        import re
        context = self.context
        
        # ç§»é™¤å¤šä½™ç©ºè¡Œ
        context = re.sub(r'\n\s*\n\s*\n', '\n\n', context)
        
        # ç§»é™¤é‡å¤çš„çŸ­è¯­
        lines = context.split('\n')
        seen_lines = set()
        unique_lines = []
        
        for line in lines:
            line_key = line.strip().lower()
            if line_key not in seen_lines and len(line_key) > 10:
                seen_lines.add(line_key)
                unique_lines.append(line)
        
        self.context = '\n'.join(unique_lines)
        
        # å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œè¿›è¡ŒäºŒæ¬¡å‹ç¼©
        if len(self.context) > target_length:
            self.context = self._secondary_compress(self.context)
        
        self.save()
    
    def _generate_compact_summary(self, conversations):
        """ç”Ÿæˆé«˜åº¦å‹ç¼©çš„æ‘˜è¦"""
        import re
        
        # åªæå–æœ€å…³é”®çš„ä¿¡æ¯
        key_info = {
            'error_codes': set(),
            'services': set(),
            'topics': set()
        }
        
        for conv in conversations:
            # æå–é”™è¯¯ç 
            error_codes = re.findall(r'ERROR\s+(\d+)', conv)
            key_info['error_codes'].update(error_codes[:3])  # æœ€å¤š3ä¸ª
            
            # æå–æœåŠ¡å
            services = re.findall(r'æœåŠ¡[ï¼š:]\s*([^\s\n]+)', conv)
            key_info['services'].update(services[:2])  # æœ€å¤š2ä¸ª
            
            # æå–ä¸»é¢˜
            if "æ•°æ®åº“" in conv:
                key_info['topics'].add("æ•°æ®åº“")
            if "ç½‘ç»œ" in conv:
                key_info['topics'].add("ç½‘ç»œ")
            if "æ€§èƒ½" in conv:
                key_info['topics'].add("æ€§èƒ½")
        
        # æ„å»ºç´§å‡‘æ‘˜è¦
        summary_parts = ["## å†å²æ‘˜è¦"]
        
        if key_info['topics']:
            summary_parts.append(f"ä¸»é¢˜: {', '.join(key_info['topics'])}")
        
        if key_info['error_codes']:
            summary_parts.append(f"é”™è¯¯ç : {', '.join(key_info['error_codes'])}")
        
        if key_info['services']:
            summary_parts.append(f"æœåŠ¡: {', '.join(key_info['services'])}")
        
        summary_parts.append(f"è½®æ•°: {len(conversations)}")
        
        return '\n'.join(summary_parts)
    
    def update_context_with_compression(self, user_input: str, bot_reply: str):
        """å¸¦å‹ç¼©çš„ä¸Šä¸‹æ–‡æ›´æ–°"""
        new_entry = f"ç”¨æˆ·ï¼š{user_input}\nå›å¤ï¼š{bot_reply}\n"
        new_context = self.context + new_entry
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
        if len(new_context) > self.max_context_length:
            self.context = new_context
            self._compress_context()
        else:
            self.context = new_context
            self.save()
    
    def get_or_create_state(self):
        """è·å–æˆ–åˆ›å»ºå¯¹è¯çŠ¶æ€"""
        from .models import ConversationState
        state, created = ConversationState.objects.get_or_create(
            session=self,
            defaults={
                'current_stage': 'problem_identification',
                'analysis_depth': 1,
                'user_satisfaction': 0
            }
        )
        return state
    
    def __str__(self):
        return self.session_id


class ConversationState(models.Model):
    """å¯¹è¯çŠ¶æ€ç®¡ç†æ¨¡å‹ï¼Œç”¨äºè·Ÿè¸ªå¯¹è¯é˜¶æ®µå’Œåˆ†æçŠ¶æ€"""
    session = models.OneToOneField(
        ConversationSession, 
        on_delete=models.CASCADE,
        related_name='state',
        help_text="å…³è”çš„å¯¹è¯ä¼šè¯"
    )
    current_stage = models.CharField(
        max_length=50, 
        default='problem_identification',
        choices=[
            ('problem_identification', 'é—®é¢˜è¯†åˆ«'),
            ('root_cause_analysis', 'æ ¹å› åˆ†æ'),
            ('solution_seeking', 'è§£å†³æ–¹æ¡ˆå¯»æ±‚'),
            ('prevention_planning', 'é¢„é˜²è§„åˆ’'),
            ('follow_up', 'è·Ÿè¿›è®¨è®º'),
        ],
        help_text="å½“å‰å¯¹è¯é˜¶æ®µ"
    )
    analysis_depth = models.IntegerField(
        default=1,
        help_text="åˆ†ææ·±åº¦ï¼Œè¡¨ç¤ºå¯¹è¯è½®æ¬¡"
    )
    user_satisfaction = models.IntegerField(
        default=0,
        help_text="ç”¨æˆ·æ»¡æ„åº¦è¯„åˆ† (0-5)"
    )
    last_analysis_result = models.JSONField(
        null=True, 
        blank=True,
        help_text="ä¸Šæ¬¡åˆ†æç»“æœï¼Œç”¨äºçŠ¶æ€è·Ÿè¸ª"
    )
    key_information = models.JSONField(
        default=dict,
        help_text="æå–çš„å…³é”®ä¿¡æ¯ï¼ˆé”™è¯¯ç ã€æœåŠ¡åç­‰ï¼‰"
    )
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    class Meta:
        verbose_name = "å¯¹è¯çŠ¶æ€"
        verbose_name_plural = "å¯¹è¯çŠ¶æ€"
    
    def update_stage(self, new_stage: str):
        """æ›´æ–°å¯¹è¯é˜¶æ®µ"""
        self.current_stage = new_stage
        self.analysis_depth += 1
        self.save()
    
    def update_satisfaction(self, satisfaction: int):
        """æ›´æ–°ç”¨æˆ·æ»¡æ„åº¦"""
        if 0 <= satisfaction <= 5:
            self.user_satisfaction = satisfaction
            self.save()
    
    def update_key_information(self, key_info: dict):
        """æ›´æ–°å…³é”®ä¿¡æ¯"""
        self.key_information = key_info
        self.save()
    
    def __str__(self):
        return f"{self.session.session_id} - {self.current_stage}"
