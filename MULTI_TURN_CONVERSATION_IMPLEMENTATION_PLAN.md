# å¤šè½®å¯¹è¯åŠŸèƒ½å®Œæ•´å®æ–½è§„åˆ’

## ğŸ¯ **å®æ–½èŒƒå›´**

### **é«˜ä¼˜å…ˆçº§ä¼˜åŒ–ï¼ˆPhase 1ï¼‰**
1. ä¸Šä¸‹æ–‡é•¿åº¦æ§åˆ¶
2. å¯¹è¯ç±»å‹è¯†åˆ«ä¸è‡ªé€‚åº”å“åº”
3. æ™ºèƒ½å“åº”å¤„ç†

### **ä¸­ä¼˜å…ˆçº§ä¼˜åŒ–ï¼ˆPhase 2ï¼‰**
1. å¯¹è¯çŠ¶æ€ç®¡ç†
2. ä¸»åŠ¨å¯¹è¯å¼•å¯¼
3. ä¸Šä¸‹æ–‡è´¨é‡ä¼˜åŒ–

## ğŸ“‹ **è¯¦ç»†å®æ–½è§„åˆ’**

### **Phase 1: åŸºç¡€ä¼˜åŒ–ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰**

#### **Step 1.1: æ•°æ®æ¨¡å‹æ‰©å±•**
**ç›®æ ‡**: ä¸ºå¤šè½®å¯¹è¯åŠŸèƒ½æä¾›æ•°æ®åŸºç¡€
**é¢„è®¡æ—¶é—´**: 1-2å°æ—¶

**å…·ä½“ä»»åŠ¡**:
1. æ‰©å±•`ConversationSession`æ¨¡å‹
```python
class ConversationSession(models.Model):
    # ç°æœ‰å­—æ®µ
    session_id = models.CharField(max_length=100)
    user = models.ForeignKey(APIKey, on_delete=models.CASCADE)
    context = models.TextField(blank=True)
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    # æ–°å¢å­—æ®µ
    context_summary = models.TextField(blank=True)  # ä¸Šä¸‹æ–‡æ‘˜è¦
    recent_context = models.TextField(blank=True)   # æœ€è¿‘Nè½®å¯¹è¯
    max_context_length = models.IntegerField(default=4000)  # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
    conversation_type = models.CharField(max_length=50, default='fault_analysis')  # å¯¹è¯ç±»å‹
```

2. åˆ›å»º`ConversationState`æ¨¡å‹
```python
class ConversationState(models.Model):
    session = models.OneToOneField(ConversationSession, on_delete=models.CASCADE)
    current_stage = models.CharField(max_length=50, default='problem_identification')  # å½“å‰é˜¶æ®µ
    analysis_depth = models.IntegerField(default=1)  # åˆ†ææ·±åº¦
    user_satisfaction = models.IntegerField(default=0)  # ç”¨æˆ·æ»¡æ„åº¦
    last_analysis_result = models.JSONField(null=True, blank=True)  # ä¸Šæ¬¡åˆ†æç»“æœ
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
```

3. æ•°æ®åº“è¿ç§»
```bash
python manage.py makemigrations
python manage.py migrate
```

#### **Step 1.2: å¯¹è¯ç±»å‹è¯†åˆ«ç³»ç»Ÿ**
**ç›®æ ‡**: å®ç°æ™ºèƒ½å¯¹è¯ç±»å‹è¯†åˆ«
**é¢„è®¡æ—¶é—´**: 2-3å°æ—¶

**å…·ä½“ä»»åŠ¡**:
1. åœ¨`topklogsystem.py`ä¸­æ·»åŠ å¯¹è¯ç±»å‹æšä¸¾
```python
from enum import Enum

class ConversationType(Enum):
    FAULT_ANALYSIS = "fault_analysis"        # æ•…éšœåˆ†æï¼ˆJSONæ ¼å¼ï¼‰
    GENERAL_QUESTION = "general_question"    # ä¸€èˆ¬é—®é¢˜ï¼ˆMarkdownæ ¼å¼ï¼‰
    FOLLOW_UP_QUESTION = "follow_up"         # è·Ÿè¿›é—®é¢˜ï¼ˆMarkdownæ ¼å¼ï¼‰
    EXPLANATION_REQUEST = "explanation"       # è§£é‡Šè¯·æ±‚ï¼ˆMarkdownæ ¼å¼ï¼‰
    PREVENTION_QUESTION = "prevention"       # é¢„é˜²æªæ–½ï¼ˆMarkdownæ ¼å¼ï¼‰
    DEPENDENCY_QUESTION = "dependency"       # ä¾èµ–å…³ç³»ï¼ˆMarkdownæ ¼å¼ï¼‰
```

2. å®ç°å¯¹è¯ç±»å‹è¯†åˆ«æ–¹æ³•
```python
def detect_conversation_type(self, query: str, context: str) -> ConversationType:
    """è¯†åˆ«å¯¹è¯ç±»å‹"""
    query_lower = query.lower()
    
    # æ•…éšœåˆ†æç±»é—®é¢˜
    fault_keywords = ["é”™è¯¯", "æ•…éšœ", "å¼‚å¸¸", "å¤±è´¥", "error", "fatal", "exception"]
    if any(keyword in query_lower for keyword in fault_keywords):
        if len(context) < 100:  # ç¬¬ä¸€è½®å¯¹è¯
            return ConversationType.FAULT_ANALYSIS
        else:
            return ConversationType.FOLLOW_UP_QUESTION
    
    # é¢„é˜²æªæ–½ç±»é—®é¢˜
    prevention_keywords = ["é¢„é˜²", "é¿å…", "é˜²æ­¢", "å¦‚ä½•é¿å…", "æ€ä¹ˆé¢„é˜²"]
    if any(keyword in query_lower for keyword in prevention_keywords):
        return ConversationType.PREVENTION_QUESTION
    
    # ä¾èµ–å…³ç³»ç±»é—®é¢˜
    dependency_keywords = ["ä¾èµ–", "å…³ç³»", "è°ƒç”¨", "æœåŠ¡", "ä¾èµ–å…³ç³»", "è°ƒç”¨é“¾"]
    if any(keyword in query_lower for keyword in dependency_keywords):
        return ConversationType.DEPENDENCY_QUESTION
    
    # è§£é‡Šç±»é—®é¢˜
    explanation_keywords = ["æ˜¯ä»€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å¦‚ä½•", "æ€ä¹ˆ", "ä»€ä¹ˆæ„æ€", "è§£é‡Š"]
    if any(keyword in query_lower for keyword in explanation_keywords):
        return ConversationType.EXPLANATION_REQUEST
    
    # é»˜è®¤æƒ…å†µ
    return ConversationType.GENERAL_QUESTION
```

3. æµ‹è¯•å¯¹è¯ç±»å‹è¯†åˆ«å‡†ç¡®æ€§

#### **Step 1.3: è‡ªé€‚åº”Promptæ„å»ºç³»ç»Ÿ**
**ç›®æ ‡**: ä¸ºä¸åŒå¯¹è¯ç±»å‹æ„å»ºä¸“é—¨çš„Prompt
**é¢„è®¡æ—¶é—´**: 3-4å°æ—¶

**å…·ä½“ä»»åŠ¡**:
1. é‡æ„`_build_prompt`æ–¹æ³•
```python
def _build_prompt(self, query: str, context: Dict) -> List[Dict]:
    # è¯†åˆ«å¯¹è¯ç±»å‹
    conversation_type = self.detect_conversation_type(query, context.get('context', ''))
    
    # æ ¹æ®ç±»å‹æ„å»ºä¸åŒçš„Prompt
    return self._build_adaptive_prompt(query, context, conversation_type)

def _build_adaptive_prompt(self, query: str, context: Dict, conversation_type: ConversationType) -> List[Dict]:
    """æ ¹æ®å¯¹è¯ç±»å‹æ„å»ºä¸åŒçš„Prompt"""
    
    if conversation_type == ConversationType.FAULT_ANALYSIS:
        return self._build_fault_analysis_prompt(query, context)
    elif conversation_type == ConversationType.FOLLOW_UP_QUESTION:
        return self._build_follow_up_prompt(query, context)
    elif conversation_type == ConversationType.PREVENTION_QUESTION:
        return self._build_prevention_prompt(query, context)
    elif conversation_type == ConversationType.DEPENDENCY_QUESTION:
        return self._build_dependency_prompt(query, context)
    elif conversation_type == ConversationType.EXPLANATION_REQUEST:
        return self._build_explanation_prompt(query, context)
    else:
        return self._build_general_prompt(query, context)
```

2. å®ç°å„ç§Promptæ¨¡æ¿
```python
def _build_fault_analysis_prompt(self, query: str, context: Dict) -> List[Dict]:
    """æ•…éšœåˆ†æPromptï¼ˆè¿”å›JSONï¼‰"""
    # ä½¿ç”¨ç°æœ‰çš„æ•…éšœåˆ†æPrompt
    pass

def _build_follow_up_prompt(self, query: str, context: Dict) -> List[Dict]:
    """è·Ÿè¿›é—®é¢˜Promptï¼ˆè¿”å›Markdownï¼‰"""
    system_message = f"""
    ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç³»ç»Ÿæ•…éšœè¯Šæ–­ä¸“å®¶ã€‚ç”¨æˆ·æ­£åœ¨è·Ÿè¿›ä¹‹å‰çš„æ•…éšœåˆ†æï¼Œè¯·åŸºäºä¹‹å‰çš„åˆ†æç»“æœå’Œå½“å‰é—®é¢˜ï¼Œæä¾›è¯¦ç»†çš„å›ç­”ã€‚
    
    è¯·ç”¨è‡ªç„¶è¯­è¨€å›ç­”ï¼Œä½¿ç”¨Markdownæ ¼å¼ï¼ŒåŒ…å«ï¼š
    - ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜
    - æä¾›ç›¸å…³çš„æŠ€æœ¯ç»†èŠ‚
    - ç»™å‡ºå®ç”¨çš„å»ºè®®
    - ä½¿ç”¨åˆ—è¡¨ã€ä»£ç å—ç­‰æ ¼å¼å¢å¼ºå¯è¯»æ€§
    
    ä¸è¦ä½¿ç”¨JSONæ ¼å¼ï¼Œç›´æ¥è¾“å‡ºMarkdownå†…å®¹ã€‚
    """
    # ... æ„å»ºå®Œæ•´çš„Prompt

def _build_prevention_prompt(self, query: str, context: Dict) -> List[Dict]:
    """é¢„é˜²æªæ–½Promptï¼ˆè¿”å›Markdownï¼‰"""
    system_message = f"""
    ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç³»ç»Ÿè¿ç»´ä¸“å®¶ã€‚ç”¨æˆ·è¯¢é—®å¦‚ä½•é¢„é˜²ç³»ç»Ÿæ•…éšœï¼Œè¯·æä¾›è¯¦ç»†çš„é¢„é˜²æªæ–½å»ºè®®ã€‚
    
    è¯·ç”¨Markdownæ ¼å¼å›ç­”ï¼ŒåŒ…å«ï¼š
    - é¢„é˜²æªæ–½åˆ†ç±»ï¼ˆç›‘æ§ã€é…ç½®ã€ä»£ç ã€æµç¨‹ï¼‰
    - å…·ä½“çš„å®æ–½å»ºè®®
    - æœ€ä½³å®è·µ
    - å·¥å…·æ¨è
    
    ä½¿ç”¨åˆ—è¡¨ã€è¡¨æ ¼ç­‰æ ¼å¼ç»„ç»‡å†…å®¹ã€‚
    """
    # ... æ„å»ºå®Œæ•´çš„Prompt
```

#### **Step 1.4: ä¸Šä¸‹æ–‡é•¿åº¦æ§åˆ¶**
**ç›®æ ‡**: å®ç°æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†
**é¢„è®¡æ—¶é—´**: 2-3å°æ—¶

**å…·ä½“ä»»åŠ¡**:
1. å®ç°ä¸Šä¸‹æ–‡å‹ç¼©æ–¹æ³•
```python
def compress_context(self, context: str, max_length: int = 4000) -> str:
    """æ™ºèƒ½å‹ç¼©ä¸Šä¸‹æ–‡"""
    if len(context) <= max_length:
        return context
    
    # åˆ†å‰²å¯¹è¯è½®æ¬¡
    conversations = context.split('ç”¨æˆ·ï¼š')
    if len(conversations) <= 3:
        return context
    
    # ä¿ç•™æœ€è¿‘3è½®å¯¹è¯
    recent_conversations = conversations[-3:]
    recent_context = 'ç”¨æˆ·ï¼š'.join(recent_conversations)
    
    # å‹ç¼©æ—©æœŸå¯¹è¯ä¸ºæ‘˜è¦
    early_conversations = conversations[:-3]
    summary = self._generate_context_summary(early_conversations)
    
    return f"{summary}\n\n{recent_context}"

def _generate_context_summary(self, conversations: List[str]) -> str:
    """ç”Ÿæˆä¸Šä¸‹æ–‡æ‘˜è¦"""
    # æå–å…³é”®ä¿¡æ¯ï¼šé”™è¯¯ç ã€æœåŠ¡åã€æ—¶é—´ç­‰
    key_info = {
        'error_codes': set(),
        'services': set(),
        'time_range': None
    }
    
    for conv in conversations:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å…³é”®ä¿¡æ¯
        pass
    
    return f"## å†å²å¯¹è¯æ‘˜è¦\n- æ¶‰åŠé”™è¯¯ç : {', '.join(key_info['error_codes'])}\n- æ¶‰åŠæœåŠ¡: {', '.join(key_info['services'])}"
```

2. ä¿®æ”¹ä¼šè¯æ›´æ–°é€»è¾‘
```python
def update_context_with_compression(self, user_input: str, bot_reply: str):
    """å¸¦å‹ç¼©çš„ä¸Šä¸‹æ–‡æ›´æ–°"""
    new_entry = f"ç”¨æˆ·ï¼š{user_input}\nå›å¤ï¼š{bot_reply}\n"
    new_context = self.context + new_entry
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
    if len(new_context) > self.max_context_length:
        compressed_context = self.compress_context(new_context, self.max_context_length)
        self.context = compressed_context
    else:
        self.context = new_context
    
    self.save()
```

#### **Step 1.5: æ™ºèƒ½å“åº”å¤„ç†**
**ç›®æ ‡**: æ ¹æ®å¯¹è¯ç±»å‹å¤„ç†å“åº”
**é¢„è®¡æ—¶é—´**: 2-3å°æ—¶

**å…·ä½“ä»»åŠ¡**:
1. ä¿®æ”¹`services.py`ä¸­çš„å“åº”å¤„ç†
```python
def deepseek_r1_api_call(prompt: str, conversation_type: str = "fault_analysis") -> str:
    """è°ƒç”¨å¤§æ¨¡å‹APIï¼Œæ”¯æŒä¸åŒå“åº”æ ¼å¼"""
    system = TopKLogSystem()
    query = prompt
    result = system.query(query)
    time.sleep(0.5)

    # è·å–åŸå§‹å“åº”
    raw_response = result["response"]
    
    # æ ¹æ®å¯¹è¯ç±»å‹å¤„ç†å“åº”
    if conversation_type == "fault_analysis":
        # æ•…éšœåˆ†æï¼šè½¬æ¢ä¸ºJSONæ ¼å¼çš„Markdown
        return json_to_markdown(raw_response)
    else:
        # å…¶ä»–ç±»å‹ï¼šç›´æ¥è¿”å›Markdownæ ¼å¼
        return ensure_markdown_format(raw_response)

def ensure_markdown_format(response: str) -> str:
    """ç¡®ä¿å“åº”æ˜¯Markdownæ ¼å¼"""
    # æ¸…ç†å¯èƒ½çš„JSONæ ‡è®°
    cleaned_response = response.strip()
    if cleaned_response.startswith('```json'):
        cleaned_response = cleaned_response[7:]
    if cleaned_response.startswith('```'):
        cleaned_response = cleaned_response[3:]
    if cleaned_response.endswith('```'):
        cleaned_response = cleaned_response[:-3]
    
    # æ¸…ç†HTMLæ ‡ç­¾
    cleaned_response = re.sub(r'<[^>]+>', '', cleaned_response)
    
    # å°è¯•è§£æJSONï¼Œå¦‚æœæˆåŠŸåˆ™è½¬æ¢
    try:
        json.loads(cleaned_response)
        return json_to_markdown(cleaned_response)
    except json.JSONDecodeError:
        return cleaned_response
```

2. ä¿®æ”¹APIå±‚ä¼ é€’å¯¹è¯ç±»å‹
```python
@router.post("/chat", response={200: ChatOut})
def chat(request, data: ChatIn):
    # ... ç°æœ‰é€»è¾‘ ...
    
    # è¯†åˆ«å¯¹è¯ç±»å‹
    conversation_type = detect_conversation_type(data.user_input, session.context)
    
    # è°ƒç”¨å¤§æ¨¡å‹ï¼ˆä¼ é€’å¯¹è¯ç±»å‹ï¼‰
    reply = deepseek_r1_api_call(prompt, conversation_type.value)
    
    # æ›´æ–°ä¼šè¯çŠ¶æ€
    session.conversation_type = conversation_type.value
    session.save()
    
    # ... å…¶ä½™é€»è¾‘ ...
```

### **Phase 2: æ™ºèƒ½å¢å¼ºï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰**

#### **Step 2.1: å¯¹è¯çŠ¶æ€ç®¡ç†**
**ç›®æ ‡**: å®ç°å¯¹è¯é˜¶æ®µè¯†åˆ«å’ŒçŠ¶æ€è·Ÿè¸ª
**é¢„è®¡æ—¶é—´**: 2-3å°æ—¶

**å…·ä½“ä»»åŠ¡**:
1. å®ç°å¯¹è¯é˜¶æ®µè¯†åˆ«
```python
def detect_conversation_stage(self, context: str) -> str:
    """è¯†åˆ«å¯¹è¯é˜¶æ®µ"""
    context_lower = context.lower()
    
    if any(keyword in context_lower for keyword in ["è§£å†³æ–¹æ¡ˆ", "æ€ä¹ˆè§£å†³", "å¦‚ä½•ä¿®å¤"]):
        return "solution_seeking"
    elif any(keyword in context_lower for keyword in ["åŸå› ", "ä¸ºä»€ä¹ˆ", "æ ¹å› "]):
        return "root_cause_analysis"
    elif any(keyword in context_lower for keyword in ["é¢„é˜²", "é¿å…", "é˜²æ­¢"]):
        return "prevention_planning"
    else:
        return "problem_identification"
```

2. å®ç°çŠ¶æ€æ›´æ–°é€»è¾‘
```python
def update_conversation_state(self, session: ConversationSession, analysis_result: dict):
    """æ›´æ–°å¯¹è¯çŠ¶æ€"""
    state, created = ConversationState.objects.get_or_create(
        session=session,
        defaults={
            'current_stage': 'problem_identification',
            'analysis_depth': 1,
            'user_satisfaction': 0
        }
    )
    
    # æ›´æ–°çŠ¶æ€
    state.current_stage = self.detect_conversation_stage(session.context)
    state.last_analysis_result = analysis_result
    state.analysis_depth += 1
    state.save()
```

#### **Step 2.2: ä¸»åŠ¨å¯¹è¯å¼•å¯¼**
**ç›®æ ‡**: å®ç°æ™ºèƒ½é—®é¢˜ç”Ÿæˆå’Œå»ºè®®
**é¢„è®¡æ—¶é—´**: 3-4å°æ—¶

**å…·ä½“ä»»åŠ¡**:
1. å®ç°é—®é¢˜ç”Ÿæˆæœºåˆ¶
```python
def generate_follow_up_questions(self, analysis_result: dict, conversation_type: str) -> List[str]:
    """åŸºäºåˆ†æç»“æœç”Ÿæˆåç»­é—®é¢˜"""
    questions = []
    
    if conversation_type == "fault_analysis":
        if analysis_result.get('confidence_level') == 'LOW':
            questions.append("æ‚¨èƒ½æä¾›æ›´å¤šå…³äºè¿™ä¸ªé”™è¯¯çš„è¯¦ç»†ä¿¡æ¯å—ï¼Ÿ")
        if not analysis_result.get('affected_services'):
            questions.append("è¿™ä¸ªé”™è¯¯å½±å“äº†å“ªäº›æœåŠ¡ï¼Ÿ")
        if len(analysis_result.get('solutions', {}).get('immediate_actions', [])) < 2:
            questions.append("é™¤äº†ä¸Šè¿°è§£å†³æ–¹æ¡ˆï¼Œè¿˜æœ‰å…¶ä»–å»ºè®®å—ï¼Ÿ")
    
    elif conversation_type == "prevention":
        questions.append("è¿™äº›é¢„é˜²æªæ–½çš„å®æ–½ä¼˜å…ˆçº§å¦‚ä½•ï¼Ÿ")
        questions.append("éœ€è¦å“ªäº›å·¥å…·æ¥å®æ–½è¿™äº›é¢„é˜²æªæ–½ï¼Ÿ")
    
    return questions

def should_ask_follow_up(self, current_analysis: dict) -> bool:
    """åˆ¤æ–­æ˜¯å¦éœ€è¦è¯¢é—®åç»­é—®é¢˜"""
    return (
        current_analysis.get('confidence_level') == 'LOW' or
        len(current_analysis.get('solutions', {}).get('immediate_actions', [])) < 2 or
        not current_analysis.get('monitoring_recommendations')
    )
```

2. å®ç°ä¸»åŠ¨å»ºè®®æœºåˆ¶
```python
def generate_proactive_suggestions(self, session: ConversationSession) -> List[str]:
    """ç”Ÿæˆä¸»åŠ¨å»ºè®®"""
    suggestions = []
    
    # åŸºäºå¯¹è¯å†å²ç”Ÿæˆå»ºè®®
    if "error" in session.context.lower():
        suggestions.append("å»ºè®®æ£€æŸ¥ç›¸å…³æœåŠ¡çš„ç›‘æ§æŒ‡æ ‡")
        suggestions.append("å¯ä»¥è€ƒè™‘æŸ¥çœ‹é”™è¯¯æ—¥å¿—çš„æ—¶é—´åˆ†å¸ƒ")
    
    return suggestions
```

#### **Step 2.3: ä¸Šä¸‹æ–‡è´¨é‡ä¼˜åŒ–**
**ç›®æ ‡**: æå‡ä¸Šä¸‹æ–‡ç›¸å…³æ€§å’Œä¿¡æ¯ä»·å€¼
**é¢„è®¡æ—¶é—´**: 3-4å°æ—¶

**å…·ä½“ä»»åŠ¡**:
1. å®ç°å…³é”®ä¿¡æ¯æå–
```python
def extract_key_information(self, context: str) -> dict:
    """ä»å¯¹è¯å†å²ä¸­æå–å…³é”®ä¿¡æ¯"""
    key_info = {
        'error_codes': set(),
        'services': set(),
        'time_range': None,
        'severity_level': None,
        'keywords': set()
    }
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–é”™è¯¯ç 
    error_pattern = r'ERROR\s+(\d+)'
    error_codes = re.findall(error_pattern, context)
    key_info['error_codes'].update(error_codes)
    
    # æå–æœåŠ¡å
    service_pattern = r'æœåŠ¡[ï¼š:]\s*([^\s\n]+)'
    services = re.findall(service_pattern, context)
    key_info['services'].update(services)
    
    # æå–å…³é”®è¯
    keywords = self._extract_keywords(context)
    key_info['keywords'].update(keywords)
    
    return key_info

def _extract_keywords(self, text: str) -> List[str]:
    """æå–å…³é”®è¯"""
    # ä½¿ç”¨ç®€å•çš„å…³é”®è¯æå–
    keywords = []
    important_words = ["é”™è¯¯", "æ•…éšœ", "å¼‚å¸¸", "å¤±è´¥", "æœåŠ¡", "æ•°æ®åº“", "ç½‘ç»œ"]
    
    for word in important_words:
        if word in text:
            keywords.append(word)
    
    return keywords
```

2. å®ç°ä¸Šä¸‹æ–‡ç›¸å…³æ€§è¯„ä¼°
```python
def assess_context_relevance(self, historical_context: str, current_query: str) -> float:
    """è¯„ä¼°å†å²ä¸Šä¸‹æ–‡ä¸å½“å‰æŸ¥è¯¢çš„ç›¸å…³æ€§"""
    # ç®€å•çš„å…³é”®è¯åŒ¹é…è¯„ä¼°
    historical_keywords = self._extract_keywords(historical_context)
    current_keywords = self._extract_keywords(current_query)
    
    # è®¡ç®—å…³é”®è¯é‡å åº¦
    overlap = len(set(historical_keywords) & set(current_keywords))
    total = len(set(historical_keywords) | set(current_keywords))
    
    if total == 0:
        return 0.0
    
    return overlap / total
```

3. å®ç°æ™ºèƒ½ä¸Šä¸‹æ–‡è¿‡æ»¤
```python
def filter_relevant_context(self, context: str, current_query: str, threshold: float = 0.3) -> str:
    """è¿‡æ»¤ç›¸å…³ä¸Šä¸‹æ–‡"""
    # åˆ†å‰²å¯¹è¯è½®æ¬¡
    conversations = context.split('ç”¨æˆ·ï¼š')
    
    relevant_conversations = []
    for conv in conversations:
        if self.assess_context_relevance(conv, current_query) >= threshold:
            relevant_conversations.append(conv)
    
    return 'ç”¨æˆ·ï¼š'.join(relevant_conversations)
```

## ğŸ“… **å®æ–½æ—¶é—´è¡¨**

### **Week 1: Phase 1 åŸºç¡€ä¼˜åŒ–**
- **Day 1-2**: Step 1.1 æ•°æ®æ¨¡å‹æ‰©å±•
- **Day 3-4**: Step 1.2 å¯¹è¯ç±»å‹è¯†åˆ«ç³»ç»Ÿ
- **Day 5-7**: Step 1.3 è‡ªé€‚åº”Promptæ„å»ºç³»ç»Ÿ

### **Week 2: Phase 1 å®Œæˆ + Phase 2 å¼€å§‹**
- **Day 1-2**: Step 1.4 ä¸Šä¸‹æ–‡é•¿åº¦æ§åˆ¶
- **Day 3-4**: Step 1.5 æ™ºèƒ½å“åº”å¤„ç†
- **Day 5-7**: Step 2.1 å¯¹è¯çŠ¶æ€ç®¡ç†

### **Week 3: Phase 2 æ™ºèƒ½å¢å¼º**
- **Day 1-3**: Step 2.2 ä¸»åŠ¨å¯¹è¯å¼•å¯¼
- **Day 4-7**: Step 2.3 ä¸Šä¸‹æ–‡è´¨é‡ä¼˜åŒ–

### **Week 4: æµ‹è¯•å’Œä¼˜åŒ–**
- **Day 1-3**: åŠŸèƒ½æµ‹è¯•å’Œbugä¿®å¤
- **Day 4-5**: æ€§èƒ½ä¼˜åŒ–
- **Day 6-7**: ç”¨æˆ·ä½“éªŒä¼˜åŒ–

## ğŸ¯ **æˆåŠŸæ ‡å‡†**

### **Phase 1 æˆåŠŸæ ‡å‡†**
- âœ… æ”¯æŒ6ç§å¯¹è¯ç±»å‹è¯†åˆ«ï¼Œå‡†ç¡®ç‡>90%
- âœ… ä¸Šä¸‹æ–‡é•¿åº¦æ§åˆ¶åœ¨4000å­—ç¬¦ä»¥å†…
- âœ… ä¸åŒå¯¹è¯ç±»å‹è¿”å›åˆé€‚æ ¼å¼çš„å“åº”
- âœ… æ•…éšœåˆ†æé—®é¢˜è¿”å›JSONæ ¼å¼æŠ¥å‘Š
- âœ… å…¶ä»–é—®é¢˜è¿”å›è‡ªç„¶è¯­è¨€Markdownå›ç­”

### **Phase 2 æˆåŠŸæ ‡å‡†**
- âœ… èƒ½å¤Ÿè¯†åˆ«å¯¹è¯é˜¶æ®µå¹¶è°ƒæ•´ç­–ç•¥
- âœ… èƒ½å¤Ÿç”Ÿæˆç›¸å…³çš„åç»­é—®é¢˜
- âœ… ä¸Šä¸‹æ–‡ç›¸å…³æ€§è¯„ä¼°å‡†ç¡®ç‡>80%
- âœ… å…³é”®ä¿¡æ¯æå–å‡†ç¡®ç‡>85%
- âœ… ç”¨æˆ·ä½“éªŒæ˜¾è‘—æå‡

## ğŸš€ **æ€»ç»“**

è¿™ä¸ªå®æ–½è§„åˆ’æ¶µç›–äº†é«˜ä¼˜å…ˆçº§å’Œä¸­ä¼˜å…ˆçº§çš„æ‰€æœ‰ä¼˜åŒ–ï¼ŒæŒ‰ç…§ä¾èµ–å…³ç³»å’Œé‡è¦æ€§æ’åºï¼š

1. **åŸºç¡€ä¼˜åŒ–**ï¼šæ•°æ®æ¨¡å‹ â†’ ç±»å‹è¯†åˆ« â†’ Promptæ„å»º â†’ ä¸Šä¸‹æ–‡æ§åˆ¶ â†’ å“åº”å¤„ç†
2. **æ™ºèƒ½å¢å¼º**ï¼šçŠ¶æ€ç®¡ç† â†’ ä¸»åŠ¨å¼•å¯¼ â†’ è´¨é‡ä¼˜åŒ–

**é¢„è®¡æ€»æ—¶é—´**ï¼š3-4å‘¨
**æ ¸å¿ƒä»·å€¼**ï¼šè§£å†³å“åº”æ ¼å¼å•ä¸€åŒ–é—®é¢˜ï¼Œæå‡å¤šè½®å¯¹è¯ä½“éªŒ
**å…³é”®æˆåŠŸå› ç´ **ï¼šå¯¹è¯ç±»å‹è¯†åˆ«çš„å‡†ç¡®æ€§ï¼Œä¸Šä¸‹æ–‡ç®¡ç†çš„æ™ºèƒ½åŒ–

ä½ å¸Œæœ›æˆ‘å¼€å§‹å®æ–½å“ªä¸ªæ­¥éª¤ï¼Ÿ
